{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from Bio import Entrez\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bio\n",
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(keyword):\n",
    "    '''\n",
    "    returns IDList of research articles related the keyword\n",
    "    \n",
    "    Arg:\n",
    "        keyword (str): keyword of the interest\n",
    "        \n",
    "    return:\n",
    "        IDList (Dict): List of publication IDs related to the keywords\n",
    "    '''\n",
    "    \n",
    "    Entrez.email = 'stawar59@gmail.com'\n",
    "    pyapim = Entrez.esearch(db='pubmed', \n",
    "                            sort='relevance', \n",
    "                            retmax='1000',\n",
    "                            retmode='xml', \n",
    "                            term=keyword)\n",
    "    lst_id = Entrez.read(pyapim)\n",
    "    return lst_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_details(id_list):\n",
    "    '''\n",
    "    returns article information from pubmed\n",
    "    \n",
    "    Arg: \n",
    "        id_list (dict): id list of publications returned from serach function\n",
    "        \n",
    "    return:\n",
    "        results (dict): full information of articles \n",
    "    '''\n",
    "    iDs = ','.join(id_list)\n",
    "    Entrez.email = 'stawar59@gmail.com'\n",
    "    pyapim = Entrez.efetch(db='pubmed',\n",
    "                           retmode='xml',\n",
    "                           id=iDs)\n",
    "    opt = Entrez.read(pyapim)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Author_list(papers):\n",
    "    print(papers['PubmedArticle'])\n",
    "    author_lst=[i['MedlineCitation']['Article']['AuthorList']['CoauthorList']\\\n",
    "                      for i in papers['PubmedArticle']]\n",
    "    dff=[pd.DataFrame(author_lst[i]) for i in range(len(author_lst))]\n",
    "    names_df=pd.concat(dff, axis=0, sort=True )\n",
    "    author_df=names_df[['ForeName', 'LastName', 'CoauthorList']]\\\n",
    "                    .groupby(['ForeName', 'LastName','CoauthorList']).size()\\\n",
    "                    .reset_index(name='count').sort_values(by='count', ascending=False)\n",
    "    top=author_df.head(10)\n",
    "    google_url='https://scholar.google.co.kr/scholar?hl=ko&as_sdt=0%2C5&q='\n",
    "    name=top['ForeName']+' '+top['LastName']+' '+top['CoauthorList']\n",
    "    opt=top.reset_index(drop=True).join(pd.DataFrame({'Google Scholar':[google_url+i for i in name.str.replace(' ', '+')+'+review&oq=']}))\n",
    "    \n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_from_papers(papers):\n",
    "    fetch_key_word_papers=[i['MedlineCitation']['KeywordList'] for i in papers['PubmedArticle']]\n",
    "    lst_key_papers=list(itertools.chain.from_iterable(list(itertools.chain.from_iterable(fetch_key_word_papers))))\n",
    "    paper_ki=pd.DataFrame({'key word from papers':lst_key_papers})\n",
    "    kiword=paper_ki['key word from papers'].str.lower()\n",
    "    return kiword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_key(papers):\n",
    "    tlt_mod=[i['MedlineCitation']['Article']['ArticleTitle'].lower()\\\n",
    "            .replace(',','').replace('.','').replace(':', '').replace('?','')\\\n",
    "            .replace('<sub>', '').replace('</sub>','').replace('<sup>','').replace('</sup>','')\\\n",
    "            .replace('<i>','').replace('</i>','')\\\n",
    "            .replace(search_word.lower(),'') for i in papers['PubmedArticle']]\n",
    "    model=TfidfVectorizer(ngram_range=(2,2),stop_words='english')\n",
    "    X=model.fit_transform(tlt_mod)\n",
    "    model_df=pd.DataFrame(X.todense(), columns=sorted(model.vocabulary_))\n",
    "    key_idx=model_df.sum().sort_values(ascending=False)\n",
    "    \n",
    "    return key_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Abstract_key(papers):\n",
    "    \n",
    "    abrlst=[]\n",
    "    for i in papers['PubmedArticle']:\n",
    "        try:\n",
    "            abrlst.append(i['MedlineCitation']['Article']['Abstract']['AbstractText'][0].lower()\\\n",
    "            .replace(',','').replace('.','').replace(':', '').replace('?','')\\\n",
    "            .replace('<sub>', '').replace('</sub>','').replace('<sup>','').replace('</sup>','')\\\n",
    "            .replace('<i>','').replace('</i>','').replace(search_word.lower(),''))\n",
    "        except:\n",
    "            continue\n",
    "    model=TfidfVectorizer(ngram_range=(2,2),stop_words='english')\n",
    "    X=model.fit_transform(abrlst)\n",
    "    model_df=pd.DataFrame(X.todense(), columns=sorted(model.vocabulary_))\n",
    "    key_idx=model_df.sum().sort_values(ascending=False)\n",
    "    \n",
    "    return key_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_word='bioactive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Entrez' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-6a1f78155572>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mid_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'IdList'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpapers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_details\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-6080e5d403ef>\u001b[0m in \u001b[0;36msearch\u001b[1;34m(keyword)\u001b[0m\n\u001b[0;32m     10\u001b[0m     '''\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mEntrez\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memail\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'stawar59@gmail.com'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     pyapim = Entrez.esearch(db='pubmed', \n\u001b[0;32m     14\u001b[0m                             \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relevance'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Entrez' is not defined"
     ]
    }
   ],
   "source": [
    "results = search(search_word)\n",
    "id_list = results['IdList']\n",
    "papers = fetch_details(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=Author_list(papers)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x =\"count\", data=df)\n",
    " \n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Google Scholar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "key_paper_lst=key_from_papers(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_paper_lst=key_from_papers(papers)\n",
    "key_paper_lst.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a=key_paper_lst.value_counts().head(20)\n",
    "a=a.tolist()\n",
    "a=pd.DataFrame({\"count\":a})\n",
    "\n",
    "b= key_paper_lst.head(20)\n",
    "b=b.tolist()\n",
    "b=pd.DataFrame({\"words\":b})\n",
    "\n",
    "dff = pd.concat([b,a],axis=1)\n",
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.bar(dff, x='words', y='count')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "STOPWORDS.add('https')  # remove htps to the world Cloud\n",
    "\n",
    "def Plot_world(text):\n",
    "    \n",
    "    comment_words = ' '\n",
    "    stopwords = set(STOPWORDS) \n",
    "    \n",
    "    for val in text: \n",
    "\n",
    "        # typecaste each val to string \n",
    "        val = str(val) \n",
    "\n",
    "        # split the value \n",
    "        tokens = val.split() \n",
    "\n",
    "        # Converts each token into lowercase \n",
    "        for i in range(len(tokens)): \n",
    "            tokens[i] = tokens[i].lower() \n",
    "\n",
    "        for words in tokens: \n",
    "            comment_words = comment_words + words + ' '\n",
    "\n",
    "\n",
    "    wordcloud = WordCloud(width = 5000, height = 4000, \n",
    "                    background_color ='black', \n",
    "                    stopwords = stopwords, \n",
    "                    min_font_size = 10).generate(comment_words) \n",
    "\n",
    "    # plot the WordCloud image                        \n",
    "    plt.figure(figsize = (12, 12), facecolor = 'k', edgecolor = 'k' ) \n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = dff.words.values\n",
    "\n",
    "Plot_world(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "key_title=title_key(papers)\n",
    "key_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(key_title-key_title.mean())/key_title.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_abstract=Abstract_key(papers)\n",
    "key_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "key_abstract[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(key_abstract-key_abstract.mean())/key_abstract.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_word = \"bone\"\n",
    "search_word+' '+key_abstract.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
