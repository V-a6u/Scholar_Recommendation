user_id,author_id,author,title,year,abstract,co_author,coauthor_id
1,1000,Wolfgang Bibel,The role of logic for data and knowledge bases: a brief survey,1989,"When two or more distinct organizations interconnect their internal computer networks they form an Inter-Organization Network(ION). IONs support the exchange of cad/cam data between manufacturers and subcontractors, software distribution from vendors to users, customer input to suppliers' order-entry systems, and the shared use of expensive computational resources by research laboratories, as examples. This paper analyzes the technical implications of interconnecting networks across organization boundaries.After analyzing the organization context in which IONs are used, we demonstrate that such interconnections are not satisfied by traditional network design criteria of connectivity and transparency. To the contrary, a primary high-level requirement is access control, and participating organizations must be able to limit connectivity and make network boundaries visible. We describe a scheme based on non-discretionary control which allows interconnecting organizations to combine gateway, network, and system-level mechanisms to enforce cross-boundary control over invocation and information flow, while minimizing interference with internal operations.Access control requirements such as these impose new requirements on the underlying interconnection protocols. We demonstrate such alternative interconnection protocols that support loose coupling across administrative boundaries and that accommodate the necessary control mechanisms. Message-based gateways that support non-real-time invocation of services (e.g., file and print servers, financial transactions, VLSI design tools, etc.) are a promising basis for such loose couplings.",Jean-Marie Nicolas,1001
2,1001,Jean-Marie Nicolas,How to look at deductive databases,1989,"Hosts are under EMCON condition, short for Emission Control, when they are prohibited from transmitting any data. Under such condition, hosts can only receive data passively and can not acknowledge any data reception. This unidirectionality of the communication channel breaks down all currently defined transport protocols. In this paper, we present architectural alternatives that would support limited communication service under such condition and also discuss other open issues.",J. B. Bocca,1005
3,1002,D. Chan,An experiment with programming using pure negation,1989,"In distributed systems surveillance protocols are used for monitoring the status of remote sites. A remote site is regarded as being available as long as messages are received from this site, otherwise it is regarded as being unavailable. If a site becomes unavailable, this will be reported to other sites and recovery actions can be initiated. Using an example it will be shown that in certain cases it is necessary, that whenever some site S1 detects the unavailability of some other site S2, within a fixed amount of time S2 must also have detected an unavailability of S1. Unfortunately, this cannot be guaranteed by existing surveillance protocols. Another problem with existing protocols is, that remote sites are usually reported as being unavailable after being timed out only once, i.e. the loss of just one message might cause complete systems to back out.Two versions of a protocol for so-called symmetric surveillance are presented. Both guarantee, that if S1 detects the unavailability of S2 at time t0, then S2 (provided that S2 has not crashed) will become aware of this fact at t1 such that [t1 - t0]",M. Wallace,1003
4,1003,M. Wallace,Mapping object-oriented concepts into relational concepts by meta-compilation in a logic programming environment,1988,"The Versatile Message Transaction Protocol (VMTP) is a transport-level protocol designed to support remote procedure call, multicast and real-time communication. The protocol is optimized for efficient page-level network file access in particular.In this paper, we describe the significant aspects of the VMTP design, including the VMTP treatment of sessions, addressing, duplicate suppression, flow control and retransmissions plus its provision for multicast. The VMTP design reflects a change in the use of computer communication as well as a change in the underlying hardware base for the next generation of communication systems. It also challenges certain established notions in the design of protocols.",J. C. Freytag,1004
5,1004,J. C. Freytag,Mapping object-oriented concepts into relational concepts by meta-compilation in a logic programming environment,1988,"The paper is concerned with efficient implementation of evolved modular and structured microprogramming. A microprogrammable architecture is presented that permits designing hierarchical complicated modular microprograms at two distinct levels: the global control and the data processing level. The architecture is based on two cooperating microprogram control units that separately store and perform control and executive microinstructions and microcode modules. The control organization of an implementing computer is presented which assures the quasi time-transparency of modular control in microprograms during the microprogram execution. This is achieved by parallel functioning of constituent control units, that permits preparing in advance addresses of executive modules referenced by control microinstructions. The efficient implementation of control statements of high level languages and microprogramming at the assembler language level for the proposed architecture are also discussed in the paper.",J. B. Bocca,1005
6,1005,J. B. Bocca,Rules for implementing very large knowledge base systems,1989,"We apply the verification methodology underlying the S*-System[12], [13] to the verification of a hierarchically structured design [16] of an emulation of the instruction-set of a commercially available computer on a commercially available micro-architecture. Based on this case-study, we discuss some aspects of the relation between verification and generation of microcode.",J. C. Freytag,1004
7,1006,Andrew V. Goldberg,A new approach to the maximum-flow problem,1988,"All previously known efficient maximum-flow algorithms work by finding augmenting paths, either one path at a time (as in the original Ford and Fulkerson algorithm) or all shortest-length augmenting paths at once (using the layered network approach of Dinic). An alternative method based on the preflow concept of Karzanov is introduced. A preflow is like a flow, except that the total amount flowing into a vertex is allowed to exceed the total amount flowing out. The method maintains a preflow in the original network and pushes local flow excess toward the sink along what are estimated to be shortest paths. The algorithm and its analysis are simple and intuitive, yet the algorithm runs as fast as any other known method on dense graphs, achieving an O(n3) time bound on an n-vertex graph. By incorporating the dynamic tree data structure of Sleator and Tarjan, we obtain a version of the algorithm running in O(nm log(n2/m)) time on an n-vertex, m-edge graph. This is as fast as any known method for any graph density and faster on graphs of moderate density. The algorithm also admits efficient distributed and parallel implementations. A parallel implementation running in O(n2log n) time using n processors and O(m) space is obtained. This time bound matches that of the Shiloach-Vishkin algorithm, which also uses n processors but requires O(n2) space.",Robert E. Tarjan,1007
8,1007,Robert E. Tarjan,Erratum: an O (n log log n)-time algorithm for triangulating a simple polygon,1985,"In this article we study the amortized efficiency of the “move-to-front” and similar rules for dynamically maintaining a linear list. Under the assumption that accessing the ith element from the front of the list takes &thgr;(i) time, we show that move-to-front is within a constant factor of optimum among a wide class of list maintenance rules. Other natural heuristics, such as the transpose and frequency count rules, do not share this property. We generalize our results to show that move-to-front is within a constant factor of optimum as long as the access cost is a convex function. We also study paging, a setting in which the access cost is not convex. The paging rule corresponding to move-to-front is the “least recently used” (LRU) replacement rule. We analyze the amortized complexity of LRU, showing that its efficiency differs from that of the off-line paging rule (Belady's MIN algorithm) by a factor that depends on the size of fast memory. No on-line paging algorithm has better amortized performance.",Daniel D. Sleator,1008
9,1008,Daniel D. Sleator,A locally adaptive data compression scheme,1986,"A data compression scheme that exploits locality of reference, such as occurs when words are used frequently over short intervals and then fall into long periods of disuse, is described. The scheme is based on a simple heuristic for self-organizing sequential search and on variable-length encodings of integers. We prove that it never performs much worse than Huffman coding and can perform substantially better; experiments on real files show that its performance is usually quite close to that of Huffman coding. Our scheme has many implementation advantages: it is simple, allows fast encoding and decoding, and requires only one pass over the data to be compressed (static Huffman coding takes two passes).",Wolfgang Bibel,1000
10,1007,Robert E. Tarjan,Planar point location using persistent search trees,1986,"A classical problem in computational geometry is the planar point location problem. This problem calls for preprocessing a polygonal subdivision of the plane defined by n line segments so that, given a sequence of points, the polygon containing each point can be determined quickly on-line. Several ways of solving this problem in O(log n) query time and O(n) space are known, but they are all rather complicated. We propose a simple O(log n)-query-time, O(n)-space solution, using persistent search trees. A persistent search tree differs from an ordinary search tree in that after an insertion or deletion, the old version of the tree can still be accessed. We develop a persistent form of binary search tree that supports insertions and deletions in the present and queries in the past. The time per query or update is O(log m), where m is the total number of updates, and the space needed is O(1) per update. Our planar point location algorithm is an immediate application of this data structure. The structure also provides an alternative to Chazelle's """"hive graph"""" structure, which has a variety of applications in geometric retrieval.",Jean-Marie Nicolas,1001
11,1010,David Laur,Hierarchical splatting: a progressive refinement algorithm for volume rendering,1991,"This paper presents a progressive refinement algorithm for volume rendering which uses a pyramidal volume representation. Besides storing average values, the pyramid stores estimated error, so an octtree can be fit to the pyramid given a user-supplied precision. This octtree is then drawn using a set of splats, or footprints, each scaled to match the size of the projection of a cell. The splats themselves are approximated with RGBA Gouraud-shaded polygons, so that they can be drawn efficiently on modern graphics workstations. The result is a real-time rendering algorithm suitable for interactive applications.",D. Chan,1002
12,1011,Bruce R. Donald,Real-time robot motion planning using rasterizing computer graphics hardware,1990,"We present a real-time robot motion planner that is fast and complete to a resolution. The technique is guaranteed to find a path if one exists at the resolution, and all paths returned are safe. The planner can handle any polyhedral geometry of robot and obstacles, including disjoint and highly concave unions of polyhedra.The planner uses standard graphics hardware to rasterize configuration space obstacles into a series of bitmap slices, and then uses dynamic programming to create a navigation function (a discrete vector-valued function) and to calculate paths in this rasterized space. The motion paths which the planner produces are minimal with respect to an L1 (Manhattan) distance metric that includes rotation as well as translation.Several examples are shown illustrating the competence of the planner at generating planar rotational and translational plans for complex two and three dimensional robots. Dynamic motion sequences, including complicated and non-obvious backtracking solutions, can be executed in real time.",Donald P. Greenberg,1018
13,1012,Françis X. Sillion,A global illumination solution for general reflectance distributions,1991,"A general light transfer simulation algorithm for environments composed of materials with arbitrary reflectance functions is presented. This algorithm removes the previous practical restriction to ideal specular and/or ideal diffuse environments, and supports complex physically based reflectance distributions, This is accomplished by extending previous two-pass ray-casting radiosity approaches to handle non-uniform intensity distributions, and resolving all possible energy transfers between sample points. An implementation is described based on a spherical harmonic decomposition for encoding both bidirectional reflectance distribution functions for materials, and directional intensity distributions for illuminated surfaces. The method compares favorably with experimental measurements.",Donald P. Greenberg,1018
14,1013,Greg Turk,Interactive simulation in a multi-person virtual world,1992,"A multi-user Virtual World has been implemented combining a flexible-object simulator with a multisensory user interface, including hand motion and gestures, speech input and output, sound output, and 3-D stereoscopic graphics with head-motion parallax. The implementation is based on a distributed client/server architecture with a centralized Dialogue Manager. The simulator is inserted into the Virtual World as a server. A discipline for writing interaction dialogues provides a clear conceptual hierarchy and the encapsulation of state. This hierarchy facilitates the creation of alternative interaction scenarios and shared multiuser environment.",Lawrence Koved,1016
15,1007,Robert E. Tarjan,Faster algorithms for the shortest path problem,1990,"Efficient implementations of Dijkstra's shortest path algorithm are investigated. A new data structure, called the radix heap, is proposed for use in this algorithm. On a network with n vertices, m edges, and nonnegative integer arc costs bounded by C, a one-level form of radix heap gives a time bound for Dijkstra's algorithm of O(m + n log C). A two-level form of radix heap gives a bound of O(m + n log C/log log C). A combination of a radix heap and a previously known data structure called a Fibonacci heap gives a bound of O(m + na @@@@log C). The best previously known bounds are O(m + n log n) using Fibonacci heaps alone and O(m log log C) using the priority queue structure of Van Emde Boas et al. [ 17].",Kurt Mehlhorn,1017
16,1015,Michael Kaufmann,The Art Gallery theorem for polygons with holes,1991,"Art gallery problems which have been extensively studied over the last decade ask how to station a small (minimum) set of guards in a polygon such that every point of the polygon is watched by at least one guard. The graph-theoretic formulation and solution to the gallery problem for polygons in standard form is given. A complexity analysis is carried out, and open problems are discussed.",Kurt Mehlhorn,1017
17,1016,Lawrence Koved,Design for interactive performance in a virtual laboratory,1990,"In recent years, a number of research groups have implemented various versions of virtual world concept [2, 4, 6, 7]. A common thread among these virtual worlds is a direct manipulation user interface paradigm based on a glove device with the position and orientation of the hand registered by a tracking device. To explore this paradigm, a new project at IBM Research was started in 1989 to build a virtual laboratory for scientists and engineers. Our first step is to integrate the glove and space tracking devices with the real time graphics on a graphics superworkstation. A simple bouncing ball virtual world has been created to test underlying software and fine tune interactive performance.Our initial emphasis is placed on understanding the limitations of various system components and getting the best interactive performance from the system. With current state of technology, the glove and tracking devices can generate much more data than the graphics update process can utilize. Both the rendering process and the processes handling the device serial ports are CPU intensive. Our first design problem is how to distribute the processing and match the incoming data rates of input devices with the update rate of the graphics. After a new position from the tracker is received by the graphics, it is displayed only at the next frame update time giving the appearance that the hand image always lags behind the motion of the real hand. Our second design problem is to use techniques to compensate for this inherent lag time. This abstract describes the specific approaches we use to solve these problems and some useful insight gained in experimenting with lag time reduction by position prediction.",J. B. Bocca,1005
18,1017,Kurt Mehlhorn,On simultaneous inner and outer approximation of shapes,1990,"For compact Euclidean bodies P, Q, we define &lgr;(P, Q) to be smallest ratio r/s where r 0, s 0 satisfy sQ? ? P ? rQ?. Here sQ denotes a scaling of Q by factor s, and Q?, Q? are some translates of Q. This function &lgr; gives us a new distance function between bodies which, unlike previously studied measures, is invariant under affine transformations. If homothetic bodies are identified, the logarithm of this function is a metric. (Two bodies are homothetic if one can be obtained from the other by scaling and translation).For integer &kgr; ? 3, define &lgr;(&kgr;) to be the minimum value such that for each convex polygon P there exists a convex &kgr;-gon Q with &lgr;(P, Q) ? &lgr;(&kgr;). Among other results, we prove that 2.118… ? &lgr;(3) ? 2.25 and &lgr;(&kgr;) = 1 + &THgr;(&kgr;-2). We give an &Ogr;(n2 log2 n) time algorithm which for any input convex n-gon P, finds a triangle T that minimizes &lgr;(T, P) among triangles. But in linear time, we can find a triangle t with &lgr;(t, P) ? 2.25.Our study is motivated by the attempt to reduce the complexity of the polygon containment problem, and also the motion planning problem. In each case, we describe algorithms which will run faster when certain implicit slackness parameters of the input are bounded away from 1. These algorithms illustrate a new algorithmic paradigm in computational geometry for coping with complexity.",Rudolf Fleischer,1025
19,1018,Donald P. Greenberg,Design and simulation of opera lighting and projection effects,1991,"A major problem challenging opera designers is the inability to co-ordinate lighting, projection systems, and set designs in the preliminary planning phase. New computer graphics techniques, which provide the set and lighting designer the opportunity to evaluate, test, and control opera designs prior to the construction of full scale systems are presented. These techniques---light source input, simulation of directional lighting, modeling of scenic projection systems, and full three-dimensional simulation---show the potential for the use of computer graphics in theater design.The light source input component consists of a program for assigning light source attributes with a set of theater lighting icons. This module allows a designer to specify light source characteristics in a way familiar to the discipline and to make preliminary evaluations of the lighting conditions.An extended progressive radiosity method is introduced to simulate the directional lighting characteristics which are specified by the input program.A new projection approach is presented to simulate the optical effects of scenic projectors. In addition, a solution to the distortion problem produced by angular projections is described.The above components are integrated to produce full three-dimensional simulations of the global illumination effects in an opera scene.",Françis X. Sillion,1012
20,1019,John M. Airey,Towards image realism with interactive update rates in complex virtual building environments,1990,"Two strategies, pre-computation before display and adaptive refinement during display, are used to combine interactivity with high image quality in a virtual building simulation. Pre-computation is used in two ways. The hidden-surface problem is partially solved by automatically pre-computing potentially visible sets of the model for sets of related viewpoints. Rendering only the potentially visible subset associated with the current viewpoint, rather than the entire model, produces significant speedups on real building models. Solutions for the radiosity lighting model are pre-computed for up to twenty different sets of lights. Linear combinations of these solutions can be manipulated in real time. We use adaptive refinement to trade image realism for interactivity as the situation requires. When the user is stationary we replace a coarse model using few polygons with a more detailed model. Image-level linear interpolation smooths the transition between differing levels of image realism.","Frederick P. Brooks, Jr.",1022
21,1007,Robert E. Tarjan,Relaxed heaps: an alternative to Fibonacci heaps with applications to parallel computation,1988,"The relaxed heap is a priority queue data structure that achieves the same amortized time bounds as the Fibonacci heap—a sequence of m decrease_key and n delete_min operations takes time O(m + n log n). A variant of relaxed heaps achieves similar bounds in the worst case—O(1) time for decrease_key and O(log n) for delete_min. Relaxed heaps give a processor-efficient parallel implementation of Dijkstra's shortest path algorithm, and hence other algorithms in network optimization. A relaxed heap is a type of binomial queue that allows heap order to be violated.",D. Chan,1002
22,1021,"Frederick P. Brooks, Jr",Project GROPEHaptic displays for scientific visualization,1990,"We began in 1967 a project to develop a haptic+display for 6-D force fields of interacting protein molecules. We approached it in four stages: a 2-D system, a 3-D system tested with a simple task, a 6-D system tested with a simple task, and a full 6-D molecular docking system, our initial goal. This paper summarizes the entire project---the four systems, the evaluation experiments, the results, and our observations. The molecular docking system results are new.Our principal conclusions are:• Haptic display as an augmentation to visual display can improve perception and understanding both of force fields and of world models populated with impenetrable objects.• Whereas man-machine systems can outperform computer-only systems by orders of magnitude on some problems, haptic-augmented interactive systems seem to give about a two-fold performance improvement over purely graphical interactive systems. Better technology may give somewhat more, but a ten-fold improvement does not seem to be in the cards.• Chemists using GROPE-III can readily reproduce the true docking positions for drugs whose docking is known (but not to them) and can find very good docks for drugs whose true docks are unknown. The present tool promises to yield new chemistry research results; it is being actively used by research chemists.• The most valuable result from using GROPE-III for drug docking is probably the radically improved situation awareness that serious users report. Chemists say they have a new understanding of the details of the receptor site and its force fields, and of why a particular drug docks well or poorly.• We see various scientific/education applications for haptic displays but believe entertainment, not scientific visualization, will drive and pace the technology.• The hardware-software system technology we have used is barely adequate, and our experience sets priorities for future development.• Some unexpected perceptual phenomena were observed. All of these worked for us, not against us.","Frederick P. Brooks, Jr.",1022
23,1022,"Frederick P. Brooks, Jr.",No Silver Bullet Essence and Accidents of Software Engineering,1987,First Page of the Article,"Frederick P. Brooks, Jr.",1022
24,1023,Ware Myers,Software Pivotal to Strategic Defense,1989,"The role of software in the success of the Strategic Defense System, as reflected in the keynote address and papers at the Second International Conference on Software for Strategic Systems, in Huntsville, Alabama, October 26, 1988, is discussed. The dominant theme is that making huge aggregations of weapons work together depends on well-conceived architecture and battle-scale simulation, which takes increased investment in software environments. The issues of software reliability, software engineering environments, simulation, and software productivity are examined. An approach to software engineering called the Ada process model is defined and characterized by a set of guidelines.",Ware Myers,1023
25,1023,Ware Myers,Interactive Computer Graphics,1984,First Page of the Article,Ware Myers,1023
26,1025,Rudolf Fleischer,Approximate motion planning and the complexity of the boundary of the union of simple geometric figures,1990,"We study rigid motions of a rectangle amidst polygonal obstacles. The best known algorithms for this problem have running time &OHgr;(n2) where n is the number of obstacle corners. We introduce the tightness of a motion planning problem as a measure of the difficulty of a planning problem in an intuitive sense and describe an algorithm with running time &ogr;((a/b · 1/&egr; crit + 1)n(log n)2), where a ? b are the lengths of the sides of a rectangle and &egr;crit is the tightness of the problem. We show further that the complexity (= number of vertices) of the boundary of n bow-ties (c.f. Figure 1.1) is &Ogr;(n). Similar results for the union of other simple geometric figures such as triangles and wedges are also presented.",Michael Kaufmann,1015
27,1026,Robert L. Brown,Advanced Operating Systems,1984,First Page of the Article,Peter J. Denning,1028
28,1027,Karen A. Frenkel,"Computers, complexity, and the Statue of Liberty restoration",1986,Twentieth-century techniques such as computer-aided engineering and finite-element analysis were used to restore the nineteenth-century monument,Karen A. Frenkel,1027
29,1028,Peter J. Denning,An ACM response: the scope and directions of Computer Science,1991,"The National Research Council's Computer Science and Telecommunications Board (CSTB) chartered a two-year study on the scope and directions of computer science. As part of this study, ACM was asked to provide input on three important questions, the answers to which could have significant impact on the future direction of our discipline and profession.",Barbara Simons,1029
30,1029,Barbara Simons,An ACM response: the scope and directions of Computer Science,1991,"The National Research Council's Computer Science and Telecommunications Board (CSTB) chartered a two-year study on the scope and directions of computer science. As part of this study, ACM was asked to provide input on three important questions, the answers to which could have significant impact on the future direction of our discipline and profession.",A. Joe Turner,1093
31,1028,Peter J. Denning,A conversation with Steve Jobs,1989,"NeXT, Inc. President and CEO Steve Jobs (left), and VP of Sales and Marketing, Dan'l Lewin, discuss the goals of the new company, and the NeXT Computer System itself.",Karen A. Frenkel,1027
32,1028,Peter J. Denning,The evolution of parallel processing,1985,"As everyone knows, the computer industry is passing through a period of great change. I was speaking recently to a senior executive in one of the large companies vigorously working to meet the developing situation. The question he posed was: “Could it have been predicted?”",Peter J. Denning,1028
33,1032,Eli Upfal,How to share memory in a distributed system,1987,"The power of shared-memory in models of parallel computation is studied, and a novel distributed data structure that eliminates the need for shared memory without significantly increasing the run time of the parallel computation is described. More specifically, it is shown how a complete network of processors can deterministically simulate one PRAM step in O(log n/(log log n)2) time when both models use n processors and the size of the PRAM's shared memory is polynomial in n. (The best previously known upper bound was the trivial O(n)). It is established that this upper bound is nearly optimal, and it is proved that an on-line simulation of T PRAM steps by a complete network of processors requires &OHgr;(T(log n/ log log n)) time.A simple consequence of the upper bound is that an Ultracomputer (the currently feasible general-purpose parallel machine) can simulate one step of a PRAM (the most convenient parallel model to program) in O((log n)2log log n) steps.",Avi Wigderson,1038
34,1032,Eli Upfal,A tradeoff between space and efficiency for routing tables,1988,"Two conflicting goals play a crucial role in the design of routing schemes for communication networks. A routing scheme should use as short as possible paths for routing messages in the network, while keeping the routing information stored in the processors' local memory as succinct as possible. The efficiency of a routing scheme is measured in terms of its stretch factor - the maximum ratio between the length of a route computed by the scheme and that of a shortest path connecting the same pair of vertices.Most previous work has concentrated on finding good routing schemes (with a small fixed stretch factor) for special classes of network topologies. In this work we study the problem for general networks, and look at the entire range of possible stretch factors. The results exhibit a tradeoff between the efficiency of a routing scheme and its space requirements. We present almost tight upper and lower bounds for this tradeoff. Specifically, we prove that any routing scheme for general n-vertex networks that achieves a stretch factor k ? 1 must use a total of &OHgr;(n1+1/2k+4) bits of routing information in the networks. This lower bound is complemented by a family H(k) of hierarchical routing schemes (for every fixed k ? 1), which guarantee a stretch factor of &Ogr;(k), require storing a total of &Ogr;(n1+1/k) bits of routing information in the network and name the vertices with &Ogr;(log2 n)-bit names.",David Peleg,1040
35,1034,Manuel Blum,Checking the correctness of memories,1991,"The notion of program checking is extended to include programs that alter their environment, in particular, programs that store and retrieve data from memory. The model considered allows the checker a small amount of reliable memory. The checker is presented with a sequence of requests (online) to a data structure which must reside in a large but unreliable memory. The data structure is viewed as being controlled by an adversary. The checker is to perform each operation in the input sequence using its reliable memory and the unreliable data structure so that any error in the operation of the structure will be detected by the checker with high probability. Checkers for various data structures are presented. Lower bounds of log n on the amount of reliable memory needed by these checkers, where n is the size of the structure, are proved.",Sampath Kannan,1036
36,1035,Abhijit Ghosh,Verification of interacting sequential circuits,1991,"The problem of verifying the equivalence of interacting finite state machines (FSMs) described at the logic level is addressed. The problem is formulated as that of checking for the equivalence of the reset/starting states of the two FSMs. First, separate sum-of-product representations of the ON-sets and OFF-sets of each of the flip-flop inputs and primary outputs of the sequential circuit, are extracted using the PODEM algorithm. We describe a fast algorithm for state differentiation based on this representation. The input as well as the state space is implicitly enumerated through a process of repeated cube intersections to generate the State Transition Graph (STG).In contrast to previous approaches, this algorithm can be efficiently generalized for verifying distributed-style specifications of interacting sequential circuits, exploiting the nature of the interconnection topology. Pipeline latches in a distributed-style specification typically do not add complexity to the sequential behavior of a circuit, but greatly add to the complexity of traditional approaches to verifying sequential circuits. Pipeline latches are easily incorporated into our generalized, hierarchical verification strategy whereby the states of pipeline latches can be implicitly enumerated.Experimental results indicate the superior efficiency of this approach as compared to previous approaches for FSM verification. It is possible to verify examples with more than 1050 states.",Srinivas Devadas,1041
37,1036,Sampath Kannan,Implicit representation of graphs,1988,"How to represent a graph in memory is a fundamental data structuring question. In the usual representations of an n-node graph, the names of the nodes (i.e. integers from 1 to n) betray nothing about the graph itself. Indeed, the names (or labels) on the n nodes are just logn bit place holders to allow data on the edges to code for the structure of the graph. In our scenario, there is no such waste. By assigning &Ogr;(logn) bit labels to the nodes, we completely code for the structure of the graph, so that given the labels of two nodes we can test if they are adjacent in time linear in the size of the labels. Furthermore, given an arbitrary original labeling of the nodes, we can find structure coding labels (as above) that are no more than a small constant factor larger than the original labels. These notions are intimately related to vertex induced universal graphs of polynomial size. For example, we can label planar graphs with structure coding labels of size n. This implies the existence of a graph with n4 nodes that contains all n-node planar graphs as vertex induced subgraphs (It was not previously known that this class had polynomial sized universal graphs). The theorems on finite graphs extend to a theorem about the constrained labeling of infinite graphs.",Moni Naor,1076
38,1037,Manuel Blum,Non-interactive zero-knowledge and its applications,1988,"We show that interaction in any zero-knowledge proof can be replaced by sharing a common, short, random string. We use this result to construct the first public-key cryptosystem secure against chosen ciphertext attack.",Paul Feldman,1043
39,1038,Avi Wigderson,A fast parallel algorithm for the maximal independent set problem,1985,"A parallel algorithm is presented that accepts as input a graph G and produces a maximal independent set of vertices in G. On a P-RAM without the concurrent write or concurrent read features, the algorithm executes in O((log n)4) time and uses O((n/(log n))3) processors, where n is the number of vertices in G. The algorithm has several novel features that may find other applications. These include the use of balanced incomplete block designs to replace random sampling by deterministic sampling, and the use of a “dynamic pigeonhole principle” that generalizes the conventional pigeonhole principle.",Richard M. Karp,1094
40,1032,Eli Upfal,A time-randomness tradeoff for oblivious routing,1988,"Three parameters characterize the performance of a probabilistic algorithm: T, the runtime of the algorithm; Q, the probability that the algorithm fails to complete the computation in the first T steps and R, the amount of randomness used by the algorithm, measured by the entropy of its random source.We present a tight tradeoff between these three parameters for the problem of oblivious packet routing on N-vertex bounded-degree networks. We prove a (1 - Q) log N/T - log Q - &Ogr;(1) lower bound for the entropy of a random source of any oblivious packet routing algorithm that routes an arbitrary permutation in T steps with probability 1 - Q. We show that this lower bound is almost optimal by proving the existence, for every e3 log N ? T ? N1/2, of an oblivious algorithm that terminates in T steps with probability 1 - Q and uses (1-Q+&ogr;(1))logN/T-logQ independent random bits.We complement this result with an explicit construction of a family of oblivious algorithms that use less than a factor of log N more random bits than the optimal algorithm achieving the same run-time.",David Peleg,1040
41,1040,David Peleg,Concurrent dynamic logic,1987,"In this paper concurrent dynamic logic (CDL) is introduced as an extension of dynamic logic tailored toward handling concurrent programs. Properties of CDL are discussed, both on the propositional and first-order level, and the extension is shown to possess most of the desirable properties of DL. Its relationships with the &mgr;-calculus, game logic, DL with recursive procedures, and PTIME are further explored, revealing natural connections between concurrency, recursion, and alternation.",David Peleg,1040
42,1041,Srinivas Devadas,Verification of interacting sequential circuits,1991,"The problem of verifying the equivalence of interacting finite state machines (FSMs) described at the logic level is addressed. The problem is formulated as that of checking for the equivalence of the reset/starting states of the two FSMs. First, separate sum-of-product representations of the ON-sets and OFF-sets of each of the flip-flop inputs and primary outputs of the sequential circuit, are extracted using the PODEM algorithm. We describe a fast algorithm for state differentiation based on this representation. The input as well as the state space is implicitly enumerated through a process of repeated cube intersections to generate the State Transition Graph (STG).In contrast to previous approaches, this algorithm can be efficiently generalized for verifying distributed-style specifications of interacting sequential circuits, exploiting the nature of the interconnection topology. Pipeline latches in a distributed-style specification typically do not add complexity to the sequential behavior of a circuit, but greatly add to the complexity of traditional approaches to verifying sequential circuits. Pipeline latches are easily incorporated into our generalized, hierarchical verification strategy whereby the states of pipeline latches can be implicitly enumerated.Experimental results indicate the superior efficiency of this approach as compared to previous approaches for FSM verification. It is possible to verify examples with more than 1050 states.",David Peleg,1040
43,1042,Abhijit Ghosh,Sequential test generation at the register-transfer and logic levels,1991,"The problem of test generation for non-scan sequential VLSI circuits is addressed. A novel method of test generation that efficiently generates test sequences for stuck-at faults in the logic circuit by exploiting register-transfer-level (RTL) design information is presented. Our approach is targeted at chips with data-path like STG.The problem of sequential test generation is decomposed into three subproblems of combinational test generation, fault-free state justification and fault-free state differentiation. Standard combinational test generation algorithms are used to generate test vectors for stuck-at faults in the logic-level implementation. The required state corresponding to the test vector is justified using a fault-free justification step that is performed using the RTL specification. Similarly, if the effect of the fault has been propagated by the test vector to the flip-flop inputs alone, the faulty state produced is differentiated from the true next state by a differentiation step that uses the RTL specification.New and efficient algorithms for fault-free state justification and differentiation on RTL descriptions that contain arithmetic as well as random logic modules are described. Unlike previous approaches, this approach does not require the storage of covers or a partial STG and can be used to generate tests for entire chips without scan. Exploiting RTL information, together with a new conflict resolution technique results in improvements of up to 100X in performance over sequential test generation techniques restricted to operate at the logic level. We have successfully generated tests for the viterbi speech processor chip [18].",Srinivas Devadas,1041
44,1043,Paul Feldman,Non-interactive zero-knowledge and its applications,1988,"We show that interaction in any zero-knowledge proof can be replaced by sharing a common, short, random string. We use this result to construct the first public-key cryptosystem secure against chosen ciphertext attack.",Paul Feldman,1043
45,1044,Abdul A. Malik,Reduced offsets for two-level multi-valued logic minimization,1991,"The approaches to two-level logic minimization can be classified into two groups: those that use tautology for expansion of cubes and those that use the offset. Tautology based schemes are generally slower and often give somewhat inferior results, because of a limited global picture of the way in which the cube can be expanded. If the offset is used, usually the expansion can be done quickly and in a more global way because it is easier to see effective directions of expansion. The problem with this approach is that there are many functions that have a reasonable size onset and don't care set but the offset is unreasonably large. It was recently shown that for the minimization of such Boolean functions, a new approach using reduced offsets, provides the same global picture and can be computed much faster. In this paper we extend reduced offsets to logic functions with multi-valued inputs.",A. Richard Newton,1046
46,1041,Srinivas Devadas,Verification of interacting sequential circuits,1991,"The problem of verifying the equivalence of interacting finite state machines (FSMs) described at the logic level is addressed. The problem is formulated as that of checking for the equivalence of the reset/starting states of the two FSMs. First, separate sum-of-product representations of the ON-sets and OFF-sets of each of the flip-flop inputs and primary outputs of the sequential circuit, are extracted using the PODEM algorithm. We describe a fast algorithm for state differentiation based on this representation. The input as well as the state space is implicitly enumerated through a process of repeated cube intersections to generate the State Transition Graph (STG).In contrast to previous approaches, this algorithm can be efficiently generalized for verifying distributed-style specifications of interacting sequential circuits, exploiting the nature of the interconnection topology. Pipeline latches in a distributed-style specification typically do not add complexity to the sequential behavior of a circuit, but greatly add to the complexity of traditional approaches to verifying sequential circuits. Pipeline latches are easily incorporated into our generalized, hierarchical verification strategy whereby the states of pipeline latches can be implicitly enumerated.Experimental results indicate the superior efficiency of this approach as compared to previous approaches for FSM verification. It is possible to verify examples with more than 1050 states.",Paul Feldman,1043
47,1046,A. Richard Newton,Proceedings of the 28th ACM/IEEE Design Automation Conference,1991,"The notion of program checking is extended to include programs that alter their environment, in particular, programs that store and retrieve data from memory. The model considered allows the checker a small amount of reliable memory. The checker is presented with a sequence of requests (online) to a data structure which must reside in a large but unreliable memory. The data structure is viewed as being controlled by an adversary. The checker is to perform each operation in the input sequence using its reliable memory and the unreliable data structure so that any error in the operation of the structure will be detected by the checker with high probability. Checkers for various data structures are presented. Lower bounds of log n on the amount of reliable memory needed by these checkers, where n is the size of the structure, are proved.",A. Richard Newton,1046
48,1047,Andrea Casotto,Design management based on design traces,1991,"VOV is an automatic manager for VLSI design. It is based on the idea that CAD tools can leave a “trace” of their execution. The trace is represented as a bipartite directed and acyclic graph, in which the nodes represent either design data or CAD transactions. By managing and analyzing the traces, VOV offers a wide variety of services related to design management, such as coordination of team design, automatic execution of CAD transactions, capture of design history and data dependencies. All of these services are provided in a non-intrusive fashion. VOV has the notion of measurement on the design data, an ingredient which is necessary to provide even more services: tracking of design specifications, validation of design data, design estimation.",A. Richard Newton,1046
49,1048,Kanwar Jit Singh,A heuristic algorithm for the fanout problem,1991,"We present an algorithm to optimally distribute a signal to its required destinations. The choice of the buffers and the topology of the distribution tree depends on the availability of different strength gates and on the load and the required times at the destinations. The general problem is to construct a fanout-tree for a signal so that the required time constraint at the source node is met and the fanout-tree has a minimum area. Since the area constrained fanout problem is NP-complete and area is not a major consideration in present high density designs, we restrict our attention to the simpler problem of designing fast fanout circuits without any area constraint. The proposed algorithm builds the fanout tree by partitioning the fanout signals into subsets and then recursively solving each sub-problem. At each stage the algorithm generates a fanout tree that is an improvement over the previous stage. This feature allows the user to specify the improvement desired by the fanout correction process. The performance of the algorithm, when run on randomly generated distributions of required times and on real design examples, is very promising.",Alberto Sangiovanni-Vincentelli,1049
50,1049,Alberto Sangiovanni-Vincentelli,Computer architecture issues in circuit simulation,1988,"The computational complexity of approximating omega (G), the size of the largest clique in a graph G, within a given factor is considered. It is shown that if certain approximation procedures exist, then EXPTIME=NEXPTIME and NP=P.",Alberto Sangiovanni-Vincentelli,1049
51,1050,J. C. van Vliet,An evaluation of tagging,1985,"Tag handling accounts for a substantial amount of execution cost in latently typed languages such as Common LISP and Scheme, espicially on architectures that provide no special hardware support.",H. M. Gladney,1052
52,1008,Daniel D. Sleator,Amortized efficiency of list update and paging rules,1985,"In this article we study the amortized efficiency of the “move-to-front” and similar rules for dynamically maintaining a linear list. Under the assumption that accessing the ith element from the front of the list takes &thgr;(i) time, we show that move-to-front is within a constant factor of optimum among a wide class of list maintenance rules. Other natural heuristics, such as the transpose and frequency count rules, do not share this property. We generalize our results to show that move-to-front is within a constant factor of optimum as long as the access cost is a convex function. We also study paging, a setting in which the access cost is not convex. The paging rule corresponding to move-to-front is the “least recently used” (LRU) replacement rule. We analyze the amortized complexity of LRU, showing that its efficiency differs from that of the off-line paging rule (Belady's MIN algorithm) by a factor that depends on the size of fast memory. No on-line paging algorithm has better amortized performance.",Robert E. Tarjan,1007
53,1052,H. M. Gladney,Data replicas in distributed information services,1989,"In an information distribution network in which records are repeatedly read, it is cost-effective to keep read-only copies in work locations. This paper presents a method of updating replicas that need not be immediately synchronized with the source data or with each other. The method allows an arbitrary mapping from source records to replica records. It is fail-safe, maximizes workstation autonomy, and is well suited to a network with slow, unreliable, and/or expensive communications links.The algorithm is a manipulation of queries, which are represented as short encodings. When a response is generated, we record which portion of the source database was used. Later, when the source data are updated, this information is used to identify obsolete replicas. For each workstation, the identity of obsolete replicas is saved until a workstation process asks for this information. This workstation process deletes each obsolete replica, and replaces it by an up-to-date version either promptly or the next time the application asks for this particular item. Throughout, queries are grouped so that the impact of each source update transaction takes effect atomically at each workstation.Optimizations of the basic algorithm are outlined. These overlap change dissemination with user service, allow the mechanism to be hidden within the data delivery subsystem, and permit very large networks.",H. M. Gladney,1052
54,1053,M. Girault,A generalized birthday attack,1988,"We generalize the birthday attack presented by Coppersmith at Crypto'85 which defrauded a Davies-Price message authentication scheme. We first study the birthday paradox and a variant for which some convergence results and related bounds are provided. Secondly, we generalize the Davies-Price scheme and show how the Coppersmith attack can be extended to this case. As a consequence, the case p=4 with DES (important when RSA with a 512-bit modulus is used for signature) appears not to be secure enough.",M. Girault,1053
55,1054,B. Vallée,How to break Okamoto's cryptosystem by reducing lattice bases,1988,"The security of several signature schemes and cryptosystems, essentially proposed by Okamoto, is based on the difficulty of solving polynomial equations or inequations modulo n. The encryption and the decryption of these schemes are very simple when the factorisation of the modulus, a large composite number, is known.We show here that we can, for any odd n, solve, in polynomial probabilistic time, quadratic equations modulo n, even if the factorisation of n is hidden, provided we are given a sufficiently good approximation of the solutions. We thus deduce how to break Okamoto's second degree cryptosystem and we extend, in this way, Brickell's and Shamir's previous attacks.Our main tool is lattices that we use after a linearisation of the problem, and the success of our method depends on the geometrical regularity of a particular kind of lattices.Our paper is organized as follows: First we recall the problems already posed, their partial solutions and describe how our results solve extensions of these problems. We then introduce our main tool, lattices and show how their geometrical properties fit in our subject. Finally, we deduce our results. These methods can be generalized to higher dimensions.",M. Girault,1053
56,1055,L. C. Guillou,A practical zero-knowledge protocol fitted to security microprocessor minimizing both transmission and memory,1988,"Zero-knowledge interactive proofs are very promising for the problems related to the verification of identity. After their (mainly theoretical) introduction by S. Goldwasser, S. Micali and C. Rackoff (1985), A. Fiat and A. Shamir (1986) proposed a first practical solution: the scheme of Fiat-Shamir is a trade-off between the number of authentication numbers stored in each security microprocessor and the number of witness numbers to be checked at each verification.This paper proposes a new scheme which requires the storage of only one authentication number in each security microprocessor and the check of only one witness number. The needed computations are only 2 or 3 more than for the scheme of Fiat-Shamir.",Wolfgang Bibel,1000
57,1056,Michael Ben-Or,Efficient identification schemes using two prover interactive proofs,1989,"We present two efficient identification schemes based on the difficulty of solving the subset sum problem and the circuit satisfiability problem. Both schemes use the two prover model introduced by [BGKW], where the verifier (e.g the Bank) interacts with two untrusted provers (e.g two bank identification cards) who have jointly agreed on a strategy to convince the verifier of their identity. To believe the validity of their identity proving procedure, the verifier must make sure that the two provers can not communicate with each other during the course of the proof process. In addition to the simplicity and efficiency of the schemes, the resulting two prover interactive proofs can be shown to be perfect zero knowledge, making no intractability assumptions.",Shafi Goldwasser,1068
58,1057,Mariano P. Consens,The G+/GraphLog Visual Query System,1990,"The video presentation “The G+/GraphLog Visual Query System” gives an overview of the capabilities of the ongoing implementation of the G+ Visual Query System for visualizing both data and queries as graphs. The system provides an environment for expressing queries in GraphLog [Con89, CM89, CM90], as well as for browsing, displaying and editing graphs. The visual query system also supports displaying the answers in several different ways.Graphs are a very natural representation for data in many application domains, for example, transportation networks, project scheduling, parts hierarchies, family trees, concept hierarchies, and Hypertext. From a broader perspective, many databases can be naturally viewed as graphs. In particular, any relational database in which we can identify one or more sets of objects of interest and relationships between them can be represented by mapping these objects into nodes and relationships into edges. In the case of semantic and object-oriented databases, there is a natural mapping of objects to nodes and attributes to edges.GraphLog is a visual query language, based on a graph representation of both data and queries, that has evolved from the earlier language G+ [CMW87, CMW89, MW89]. GraphLog queries ask for patterns that must be present or absent in the database graph. Each such pattern, called a query graph, defines new edges that are added to the graph whenever the pattern is found. GraphLog queries are sets of query graphs, called graphical queries. If, when looking at a query graph in a graphical query, we do not find an edge label in the database, then there must exist another query graph in the graphical query defining that edge. The language also supports computing aggregate functions and summarizing along paths.The G+ Visual Query System is currently implemented in Smalltalk-80™, and runs on Sun 3, Sun 4 and Macintosh II workstations. A Graph Editor is available for editing query graphs and displaying database graphs. It supports graph “cutting and pasting”, as well as text editing of node and edge labels, node and edge repositioning and re-shaping, storage and retrieval of graphs as text files, etc. Automatic graph layout is also provided. For editing collections of graphs (such as graphical queries) a Graph Browser is available.The first answer mode supported by the G+ Visual Query System is to return as the result of a GraphLog query a graph with the new edges defined by the graphical query added to the database graph.An alternative way of visualizing answers is by high-lighting on the database graph, one at a time, the paths (or just the nodes) described by the query. This mode is particularly useful to locate interesting starting points for browsing.Rather than viewing the answers superimposed on the database graph, the user may choose to view them in a Graph Browser. The Graph Browser contains the set of subgraphs of the database graph that were found to satisfy the query.Finally, the user may select to collect all the subgraphs of the database graph that satisfy the query together into one new graph. This graph (as well as any other result graph from any of the above mentioned answer modes) in turn may be queried, providing a mechanism for iterative filtering of irrelevant information until a manageable subgraph is obtained.",Alberto O. Mendelzon,1059
59,1057,Mariano P. Consens,Low complexity aggregation in GraphLog and Datalog,1990,"We present facilities for computing aggregate functions over sets of tuples and along paths in a database graph.We show how Datalog can be extended to compute a large class of queries with aggregates without incurring the large expense of a language with general set manipulation capabilities. In particular, we aim for queries that can be executed efficiently in parallel, using the class NC and its various subclasses as formal models of low parallel complexity.Our approach retains the standard relational notion of relations as sets of tuples, not requiring the introduction of multisets. For the case where no rules are recursive, the language is exactly as expressive as Klug's first order language with aggregates. We show that this class of non-recursive programs cannot express transitive closure (unless LOGSPACE=NLOGSPACE), thus providing evidence for a widely believed but never proven folk result. We also study the expressive power and complexity of languages that support aggregation over recursion.We then describe how these facilities, as well as manipulating the length of paths in database graphs, are incorporated into our visual query language GraphLog. While GraphLog could easily be extended to handle all the queries described above, we prefer to restrict the language in a natural way to avoid explicit recursion; all recursion is expressed as transitive closure. We show that this guarantees all expressible queries are in NC. We analyze other proposals and show that they can express queries that are logspace-complete for P and thus unlikely to be parallelizable efficiently.",Mariano P. Consens,1057
60,1059,Alberto O. Mendelzon,Answering queries on embedded-complete database schemes,1987,"It has been observed that, for some database schemes, users may have difficulties retrieving correct information, even for simple queries. The problem occurs when some implicit “piece” of information, defined on some subset of a relation scheme, is not explicitly represented in the database state. In this situation, users may be required to know how the state and the constraints interact before they can retrieve the information correctly.In this paper, the formal notion of embedded-completeness is proposed, and it is shown that schemes with this property avoid the problem described above. A polynomial-time algorithm is given to test whether a database scheme is independent and embedded-complete. Under the assumption of independence, it is shown that embedded-complete schemes allow efficient computation of optimal relational algebra expressions equivalent to the X-total projection, for any set of attributes X.",M. Wallace,1003
61,1027,Karen A. Frenkel,Complexity and parallel processing: an interview with Richard Karp,1986,"In the following interview, which took place at ACM 85 in Denver, Karp discusses the relation of his work to leading-edge computing topics like parallel processing and artificial intelligence. Tracing his experience as a pioneer in highly theoretical computer science, Karp describes how the decision to go against established wisdom led to the work for which he is best known and how a colleague's findings led him to see links between two previously unrelated areas. Throughout, he stresses the exchange of ideas with colleagues that helped yield fundamental insights.",Richard Karp,1062
62,1061,Isabel F. Cruz,DOODLE: a visual language for object-oriented databases,1992,"In this paper we introduce DOODLE, a new visual and declarative language for object-oriented databases. The main principle behind the language is that it is possible to display and query the database with arbitrary pictures. We allow the user to tailor the display of the data to suit the application at hand or her preferences. We want the user-defined visualizations to be stored in the database, and the language to express all kinds of visual manipulations. For extendibility reasons, the language is object-oriented. The semantics of the language is given by a well-known deductive query language for object-oriented databases. We hope that the formal basis of our language will contribute to the theoretical study of database visualizations and visual query languages, a subject that we believe is of great interest, but largely left unexplored.",Isabel F. Cruz,1061
63,1062,Richard Karp,A randomized parallel branch-and-bound procedure,1988,"We present a universal randomized method called Local Best-First Search for parallelizing sequential branch-and-bound algorithms. The method executes on a message-passing multiprocessor system, and requires no global data structures or complex communication protocols. We show that, uniformly on all instances, the execution time of the method is unlikely to exceed a certain inherent lower bound by more than a constant factor.",Yanjun Zhang,1063
64,1063,Yanjun Zhang,On better heuristic for euclidean Steiner minimum trees (extended abstract),1991,Finding a shortest network interconnecting a given set of points in the Euclidean plane (a Steiner minimum tree) is known to be NP-hard. It is shown that there exists a polynomial-time heuristic with a performance ratio bigger than square root 3/2.,Yanjun Zhang,1063
65,1064,Yonatan Aumann,Asymptotically optimal PRAM emulation on faulty hypercubes (extended abstract),1991,"A scheme for emulating the parallel random access machine (PRAM) on a faulty hypercube is presented. All components of the hypercube, including the memory modules, are assumed to be subject to failure. The faults may occur at any time during the emulation and the system readjusts dynamically. The scheme, which rests on L.G. Valiant's BSP model (1990), is the first to achieve optimal and work-preserving PRAM emulation on a dynamically faulty network.",Michael Ben-Or,1056
66,1056,Michael Ben-Or,Completeness theorems for non-cryptographic fault-tolerant distributed computation,1988,"Every function of n inputs can be efficiently computed by a complete network of n processors in such a way that:If no faults occur, no set of size t n/2 of players gets any additional information (other than the function value),Even if Byzantine faults are allowed, no set of size t n/3 can either disrupt the computation or get additional information.Furthermore, the above bounds on t are tight!",Shafi Goldwasser,1068
67,1059,Alberto O. Mendelzon,A graphical query language supporting recursion,1987,"We define a language G for querying data represented as a labeled graph G. By considering G as a relation, this graphical query language can be viewed as a relational query language, and its expressive power can be compared to that of other relational query languages. We do not propose G as an alternative to general purpose relational query languages, but rather as a complementary language in which recursive queries are simple to formulate. The user is aided in this formulation by means of a graphical interface. The provision of regular expressions in G allows recursive queries more general than transitive closure to be posed, although the language is not as powerful as those based on function-free Horn clauses. However, we hope to be able to exploit well-known graph algorithms in evaluating recursive queries efficiently, a topic which has received widespread attention recently.",Isabel F. Cruz,1061
68,1064,Yonatan Aumann,Computing with faulty arrays,1992,"We present and O(1) slowdown emulation of a fault-free N x N two dimensional mesh with a slack of O(log N log log N) by a faulty mesh of the same size and slack. All components of the faulty mesh, including the memory modules, are assumed to be subject to failure. The faults may occur at any time during the emulation and the system readjusts dynamically.",Yonatan Aumann,1064
69,1068,Shafi Goldwasser,New paradigms for digital signatures and message authentication based on non-interactive zero knowledge proofs,1989,Using non-interactive zero knowledge proofs we provide a simple new paradigm for digital signing and message authentication secure against adaptive chosen message attack.For digital signatures we require that the non-interactive zero knowledge proofs be publicly verifiable: they should be checkable by anyone rather than directed at a particular verifier. We accordingly show how to implement noninteractive zero knowledge proofs in a network which have the property that anyone in the network can individually check correctness while the proof is zero knowledge to any sufficiently small coalition. This enables us to implement signatures which are history independent.,Mihir Bellare,1071
70,1056,Michael Ben-Or,Multi-prover interactive proofs: how to remove intractability assumptions,1988,"Quite complex cryptographic machinery has been developed based on the assumption that one-way functions exist, yet we know of only a few possible such candidates. It is important at this time to find alternative foundations to the design of secure cryptography. We introduce a new model of generalized interactive proofs as a step in this direction. We prove that all NP languages have perfect zero-knowledge proof-systems in this model, without making any intractability assumptions.The generalized interactive-proof model consists of two computationally unbounded and untrusted provers, rather than one, who jointly agree on a strategy to convince the verifier of the truth of an assertion and then engage in a polynomial number of message exchanges with the verifier in their attempt to do so. To believe the validity of the assertion, the verifier must make sure that the two provers can not communicate with each other during the course of the proof process. Thus, the complexity assumptions made in previous work, have been traded for a physical separation between the two provers.We call this new model the multi-prover interactive-proof model, and examine its properties and applicability to cryptography.",Michael Ben-Or,1056
71,1068,Shafi Goldwasser,Multiparty computation with faulty majority,1989,"We address the problem of performing a multiparty computation when more than half of the processors are cooperating Byzantine faults. We show how to compute any boolean function of n inputs distributively, preserving the privacy of inputs held by nonfaulty processors, and ensuring that faulty processors obtain the function value """"if and only if"""" the nonfaulty processors do. If the nonfaulty processors do not obtain the correct function value, they detect cheating with high probabihty. Our solution is based on a new type of verifiable secret sharing in which the secret is revealed not all at once but in small increments. This slow-revealing process ensures that all processors discover the secret at roughly the same time. Our solution assumes the existence of an oblivious transfer protocol and uses broadcast channels. We do not require that the processors have equal computing power.",Donald Beaver,1072
72,1071,Mihir Bellare,A technique for upper bounding the spectral norm with applications to learning,1992,"We present a general technique to upper bound the spectral norm of an arbitrary function. At the heart of our technique is a theorem which shows how to obtain an upper bound on the spectral norm of a decision tree given the spectral norms of the functions in the nodes of this tree. The theorem applies to trees whose nodes may compute any boolean functions. Applications are to the design of efficient learning algorithms and the construction of small depth threshold circuits (or neural nets). In particular, we present polynomial time algorithms for learning O(log n) clause DNF formulas and various classes of decision trees, all under the uniform distribution with membership queries.",Mihir Bellare,1071
73,1072,Donald Beaver,Multiparty protocols tolerating half faulty processors,1989,"We show that a complete broadcast network of n processors can evaluate any function f(x1,..., xn) at private inputs supplied by each processor, revealing no information other than the result of the function, while tolerating up to t maliciously faulty parties for 2t n. This improves the previous bound of 3t n on the tolerable number of faults [BG W88, CCD88]. We demonstrate a resilient method to multiply secretly shared values without using unproven cryptographic assumptions. The crux of our method is a new, non-cryptographic zero-knowledge technique which extends verifiable secret sharing to allow proofs based on secretly shared values. Under this method, a single party can secretly share values v1,...vm along with another secret w = P(v1,...,vm), where P is any polynomial size circuit; and she can prove to all other parties that w = P(v1,..., vm), without revealing w or any other information. Our protocols allow an exponentially small chance of error, but are provably optimal in their resilience against Byzantine faults. Furthermore, our solutions use operations over exponentially large fields, greatly reducing the amount of interaction necessary for computing natural functions.",Donald Beaver,1072
74,1056,Michael Ben-Or,Efficient identification schemes using two prover interactive proofs,1989,"We present two efficient identification schemes based on the difficulty of solving the subset sum problem and the circuit satisfiability problem. Both schemes use the two prover model introduced by [BGKW], where the verifier (e.g the Bank) interacts with two untrusted provers (e.g two bank identification cards) who have jointly agreed on a strategy to convince the verifier of their identity. To believe the validity of their identity proving procedure, the verifier must make sure that the two provers can not communicate with each other during the course of the proof process. In addition to the simplicity and efficiency of the schemes, the resulting two prover interactive proofs can be shown to be perfect zero knowledge, making no intractability assumptions.",;Shafi Goldwasser,1068
75,1071,Mihir Bellare,Languages that are easier than their proofs,1991,"Languages in NP are presented for which it is harder to prove membership interactively than it is to decide this membership. Similarly, languages where checking is harder than computing membership are presented. Under assumptions about triple-exponential time, incoherent sets in NP are constructed. Without any assumptions, incoherent sets are constructed in DSPACE (n to the log n), yielding the first uncheckable and non-random-self-reducible sets in that space.",Richard Beigel,1075
76,1075,Richard Beigel,When do extra majority gates help?,1992,"Suppose that f is computed by a constant depth circuit with 2m AND-, OR-, and NOT-gates, and m majority-gates. We prove that f is computed by a constant depth circuit with 2mo(1) AND-, OR-, and NOT-gates, and a single majority-gate, which is at the root.One consequence is that if f is computed by and AC0 circuit plus polylog majority-gates, then f is computed by a probabilistic perceptron having polylog order. Another consequence is that if f agrees with the parity function of three-fourths of all inputs, then fcannot be computed by a constant depth circuit with 2no(1) AND-, OR-, and NOT-gates, and no(1) majority-gates.",Richard Beigel,1075
77,1076,Moni Naor,Bit commitment using pseudo-randomness (extended abstract),1989,"We show how a pseudo-random generator can provide a bit commitment protocol. We also analyze the number of bits communicated when parties commit to many bits simultaneously, and show that the assumption of the existence of pseudorandom generators suffices to assure amortized O(1) bits of communication per bit commitment.",Moni Naor,1076
78,1077,Amos Fiat,Storing and searching a multikey table,1988,"We describe an implicit data structure for n multikey records that supports searching for a record, under any key, in the asymptotically optimal search time &Ogr;(log n). This improves on [Mun87] in which Munro describes an implicit data structure for the problem of storing n k-key records so that search on any key can be performed in &Ogr;(logk n(log log n)k-1) comparisons. The theoretical tools we develop also yield practical schemes that either halve the number of memory references over obvious solutions to the non-implicit version of the problem, or alternatively reduce the number of pointers involved significantly.",Moni Naor,1076
79,1076,Moni Naor,Amortized communication complexity (Preliminary version),1991,"The authors study the direct sum problem with respect to communication complexity: Consider a function f: D to (0, 1), where D contained in (0, 1)/sup n/*(0, 1)/sup n/. The amortized communication complexity of f, i.e. the communication complexity of simultaneously computing f on l instances, divided by l is studied. The authors present, both in the deterministic and the randomized model, functions with communication complexity Theta (log n) and amortized communication complexity O(1). They also give a general lower bound on the amortized communication complexity of any function f in terms of its communication complexity C(f).",Moni Naor,1076
80,1079,Eyal Kushilevitz,Randomized mutual exclusion algorithms revisited,1992,"In [4] a randomized algorithm for mutual exclusion with bounded waiting, employing a logarithmic sized shared variable, was given. Saias and Lynch [5] pointed out that the adversary scheduler postulated in the above paper can observe the behavior of processes in the interval between an opening of the critical section and the next closing of the critical section. it can then draw conclusions about values of their local variables as well as the value of the randomized round number component of the shared variable, and arrange the schedule so as to discriminate against a chosen process. This invalidates the claimed properties of the algorithm.In the present paper the algorithm in [4] is modified, using the ideas of [4], so as to overcome this difficulty, obtaining essentially the same results. Thus, as in [4], randomization yields simple algorithms for mutual-exclusion with bounded waiting, employing a shared variable of considerably smaller size than the lower-bound established in [1] for deterministic algorithms.",Michael O. Rabin,1080
81,1080,Michael O. Rabin,"Efficient dispersal of information for security, load balancing, and fault tolerance",1989,"An Information Dispersal Algorithm (IDA) is developed that breaks a file F of length L = &uharl; F&uharr; into n pieces Fi, l ? i ? n, each of length &uharl;Fi&uharr; = L/m, so that every m pieces suffice for reconstructing F. Dispersal and reconstruction are computationally efficient. The sum of the lengths &uharl;Fi&uharr; is (n/m) · L. Since n/m can be chosen to be close to l, the IDA is space efficient. IDA has numerous applications to secure and reliable storage of information in computer networks and even on single disks, to fault-tolerant and efficient transmission of information in networks, and to communications between processors in parallel computers. For the latter problem provably time-efficient and highly fault-tolerant routing on the n-cube is achieved, using just constant size buffers.",Michael O. Rabin,1080
82,1081,Benny Chor,Secret sharing over infinite domains (extended abstract),1989,"A (k, n) secret sharing scheme is a probabilistic mapping of a secret to n shares, such that ? The secret can be reconstructed from any k shares. ? No subset of k - 1 shares reveals any partial information about the secret.Various secret sharing schemes have been proposed, and applications in diverse contexts were found. In all these cases, the set of secrets and the set of shares are finite.In this paper we study the possibility of secret sharing schemes over infinite domains. The major case of interest is when the secrets and the shares are taken from a countable set, for example all binary strings. We show that no (k, n) secret sharing scheme over any countable domain exists (for any 2 k n).One consequence of this impossibility result is that no perfect private-key encryption schemes, over the set of all strings, exist. Stated informally, this means that there is no way to perfectly encrypt all strings without revealing information about their length.We contrast these results with the case where both the secrets and the shares are real numbers. Simple secret sharing schemes (and perfect private-key encryption schemes) are presented. Thus, infinity alone does not rule out the possibility of secret sharing.",Eyal Kushilevitz,1079
83,1082,Mihalis Yannakakis,On the approximation of maximum satisfiability,1992,"We present a 3/4 polynomial time approximation algorithm for the Maximum Satisfiability problem: Given a set of clauses, find a truth assignment that satisfies the maximum number of clauses. The algorithm applies to the weighted case as well, and involves nontrival application of network flow techniques.",Mihalis Yannakakis,1082
84,1082,Mihalis Yannakakis,A polynomial algorithm for the min-cut linear arrangement of trees,1985,An algorithm is presented that finds a min-cut linear arrangement of a tree in O(n log n) time. An extension of the algorithm determines the number of pebbles needed to play the black and white pebble game on a tree.,Mihalis Yannakakis,1082
85,1084,Sidney E. Harris,Design implications of a task-driven approach to unstructured cognitive tasks in office work,1985,"Previous research in modeling office activities has been primarily oriented toward office work that is structured and organized. In this paper we report on efforts to develop a new methodology for needs assessment evaluation. We use the Critical Task Method to identify the “bottleneck cognitive tasks” of principals with an unstructured work profile. Data were collected on the computer-support needs of faculty researchers, and the findings indicate that a “knowledge-based” design offers the most promise for delivering effective support. In addition, the systems design suggests the integration of text, data, voice, and images.",Sidney E. Harris,1084
86,1084,Sidney E. Harris,Firm size and the information technology investment intensity of life insurers,1991,"This article is organized around two research questions: (1) do small insurers exhibit a higher degree of information technology investment intensity (i.e., the ratio of information technology expense to total operating expense) than large insurers? and (2) to what extent dos the level of spending on information technology explain the degree of information technology investment intensity? The article offers an interpretation of the dependent and independent variables and uses data obtained from the life insurance industry. The findings on the whole indicate that small insurers spend a larger proportion of their operating expenses on information technology than do large insurers. Given the conditions prevailing in the life insurance industry, this means that large firms were not leaders in realizing the full potential of the economic benefits available. Contrary to expectations, spending more on information technology does not lead to a higher ratio of information technology expense to total operating expense. This finding is consistent with the observation by several academics and practitioners that how the technology is used and managed is of equal if not more important consideration than the level of spending.",Sidney E. Harris,1084
87,1086,P. S. P. Wang,A Fast and Flexible Thinning Algorithm,1989,"A fast serial and parallel algorithm for thinning digital patterns is presented. The processing speed is faster than previous algorithms in that it reads pixels along the edge of the input pattern rather than all pixels in each iteration. Using this algorithm, an experiment is conducted and the patterns such as 'X', 'H', 'A', 'moving body', and 'leaf' are tested. The results show that this algorithm is faster, structure-preserving, and more flexible in that it can be done either sequentially or in parallel.",H. E. Lü,1087
88,1087,H. E. Lü,A comment on “a fast parallel algorithm for thinning digital patterns,1986,"A fast parallel thinning algorithm for digital patterns is presented. This algorithm is an improved version of the algorithms introduced by Zhang and Suen [5] and Stefanelli and Rosenfeld [3]. An experiment using an Apple II and an Epson printer was conducted. The results show that the improved algorithm overcomes some of the disadvantages found in [5] by preserving necessary and essential structures for certain patterns which should not be deleted and maintains very fast speed, from about 1.5 to 2.3 times faster than the four-step and two-step methods described in [3] although the resulting skeletons look basically the same.",P. S. P. Wang,1086
89,1088,James H. Morris,Andrew: a distributed personal computing environment,1986,"The Information Technology Center (ITC), a collaborative effort between IBM and Carnegie-Mellon University, is in the process of creating Andrew, a prototype computing and communication system for universities. This article traces the origins of Andrew, discusses its goals and strategies, and gives an overview of the current status of its implementation and usage.",Mahadev Satyanarayanan,1089
90,1089,Mahadev Satyanarayanan,Parallel Communication in a Large Distributed Environment,1990,"The evolution of MultiRPC, a parallel remote procedure call mechanism implemented in Unix is described. Parallelism is obtained from the concurrency of processing on servers and from the overlapping of retransmissions and timeouts. Each of the parallel calls retains the semantics and functionality of the standard remote procedure calls. The underlying communication medium need not support multicast or broadcast transmissions. An analytic model of the system is derived and validated. The experimental observations demonstrate the feasibility of using MultiRPC to contact up to 100 servers in parallel.",Mahadev Satyanarayanan,1089
91,1090,Mee-Chow Chiang,"Experience with mean value analysis model for evaluating shared bus, throughput-oriented multiprocessors",1991,"We report on our experience with the accuracy of mean value analysis analytical models for evaluating shared bus multiprocessors operating in a throughput-oriented environment. Having developed separate models for multiprocessors with circuit switched and split transaction, pipelined (packet switched) buses, wc compare the results of the models with those of an actual trace-driven simulation for 5,376 multiprocessor configurations.We find that the analytical models are accurate in predicting the individual processor throughputs and partial bus utilizations. For processor throughput, the difference between the results of the models and simulation are within 1% for 75% of the cases and within 3% in 94% of all cases. For partial bus utilization the model results are with 1% of simulation results in 70% of all cases and within 3% in 92% of all cases. The models are less accurate in predicting cache miss latency.",Gurindar S. Sohi,1091
92,1091,Gurindar S. Sohi,Tradeoffs in instruction format design for horizontal architectures,1989,"With recent improvements in software techniques and the enhanced level of fine grain parallelism made available by such techniques, there has been an increased interest in horizontal architectures and large instruction words that are capable of issuing more that one operation per instruction. This paper investigates some issues in the design of such instruction formats. We study how the choice of an instruction format is influenced by factors such as the degree of pipelining and the instruction's view of the register file. Our results suggest that very large instruction words capable of issuing one operation to each functional unit resource in a horizontal architecture may be overkill. Restricted instruction formats with limited operation issuing capabilities are capable of providing similar performance (measured by the total number of time steps) with significantly less hardware in many cases.",Gurindar S. Sohi,1091
93,1091,Gurindar S. Sohi,"Instruction Issue Logic for High-Performance, Interruptible, Multiple Functional Unit, Pipelined Computers",1990,"The problems of data dependency resolution and precise interrupt implementation in pipelined processors are combined. A design for a hardware mechanism that resolves dependencies dynamically and, at the same time, guarantees precise interrupts is presented. Simulation studies show that by resolving dependencies the proposed mechanism is able to obtain a significant speedup over a simple instruction issue mechanism as well as implement precise interrupts.",Gurindar S. Sohi,1091
94,1093,A. Joe Turner,An ACM response: the scope and directions of Computer Science,1991,"The National Research Council's Computer Science and Telecommunications Board (CSTB) chartered a two-year study on the scope and directions of computer science. As part of this study, ACM was asked to provide input on three important questions, the answers to which could have significant impact on the future direction of our discipline and profession.",Paul Feldman,1043
95,1094,Richard M. Karp,Efficient PRAM simulation on a distributed memory machine,1992,"We present a randomized simulation of a nlog log (n) log (n)-processor shared memory machine (DMM) with optimal expected delay O(log log (n)) per step of simulation. The time bound for the delay is guaranteed with overwhelming probability. The algorithm is based on hashing and uses a novel simulation scheme. The best previous simulations use a simpler scheme based on hashing and have much larger expected delay: &THgr;(log(n)/log log (n)) for the simulation of an n-processor PRAM on an n processor DMM, and &THgr;(log(n)) in the case where the simulation preserves the processor-time product.",Andrew V. Goldberg,1006
96,1040,David Peleg,Concurrent dynamic logic,1987,"In this paper concurrent dynamic logic (CDL) is introduced as an extension of dynamic logic tailored toward handling concurrent programs. Properties of CDL are discussed, both on the propositional and first-order level, and the extension is shown to possess most of the desirable properties of DL. Its relationships with the &mgr;-calculus, game logic, DL with recursive procedures, and PTIME are further explored, revealing natural connections between concurrency, recursion, and alternation.",David Peleg,1040
97,1094,Richard M. Karp,A fast parallel algorithm for the maximal independent set problem,1985,"A parallel algorithm is presented that accepts as input a graph G and produces a maximal independent set of vertices in G. On a P-RAM without the concurrent write or concurrent read features, the algorithm executes in O((log n)4) time and uses O((n/(log n))3) processors, where n is the number of vertices in G. The algorithm has several novel features that may find other applications. These include the use of balanced incomplete block designs to replace random sampling by deterministic sampling, and the use of a “dynamic pigeonhole principle” that generalizes the conventional pigeonhole principle.",Avi Wigderson,1038
98,1038,Avi Wigderson,Multi-prover interactive proofs: how to remove intractability assumptions,1988,"Quite complex cryptographic machinery has been developed based on the assumption that one-way functions exist, yet we know of only a few possible such candidates. It is important at this time to find alternative foundations to the design of secure cryptography. We introduce a new model of generalized interactive proofs as a step in this direction. We prove that all NP languages have perfect zero-knowledge proof-systems in this model, without making any intractability assumptions.The generalized interactive-proof model consists of two computationally unbounded and untrusted provers, rather than one, who jointly agree on a strategy to convince the verifier of the truth of an assertion and then engage in a polynomial number of message exchanges with the verifier in their attempt to do so. To believe the validity of the assertion, the verifier must make sure that the two provers can not communicate with each other during the course of the proof process. Thus, the complexity assumptions made in previous work, have been traded for a physical separation between the two provers.We call this new model the multi-prover interactive-proof model, and examine its properties and applicability to cryptography.",David Peleg,1040
99,1040,David Peleg,A tradeoff between space and efficiency for routing tables,1988,"Two conflicting goals play a crucial role in the design of routing schemes for communication networks. A routing scheme should use as short as possible paths for routing messages in the network, while keeping the routing information stored in the processors' local memory as succinct as possible. The efficiency of a routing scheme is measured in terms of its stretch factor - the maximum ratio between the length of a route computed by the scheme and that of a shortest path connecting the same pair of vertices.Most previous work has concentrated on finding good routing schemes (with a small fixed stretch factor) for special classes of network topologies. In this work we study the problem for general networks, and look at the entire range of possible stretch factors. The results exhibit a tradeoff between the efficiency of a routing scheme and its space requirements. We present almost tight upper and lower bounds for this tradeoff. Specifically, we prove that any routing scheme for general n-vertex networks that achieves a stretch factor k ? 1 must use a total of &OHgr;(n1+1/2k+4) bits of routing information in the networks. This lower bound is complemented by a family H(k) of hierarchical routing schemes (for every fixed k ? 1), which guarantee a stretch factor of &Ogr;(k), require storing a total of &Ogr;(n1+1/k) bits of routing information in the network and name the vertices with &Ogr;(log2 n)-bit names.",Eli Upfal,1032
100,1032,Eli Upfal,How to share memory in a distributed system,1987,"The power of shared-memory in models of parallel computation is studied, and a novel distributed data structure that eliminates the need for shared memory without significantly increasing the run time of the parallel computation is described. More specifically, it is shown how a complete network of processors can deterministically simulate one PRAM step in O(log n/(log log n)2) time when both models use n processors and the size of the PRAM's shared memory is polynomial in n. (The best previously known upper bound was the trivial O(n)). It is established that this upper bound is nearly optimal, and it is proved that an on-line simulation of T PRAM steps by a complete network of processors requires &OHgr;(T(log n/ log log n)) time.A simple consequence of the upper bound is that an Ultracomputer (the currently feasible general-purpose parallel machine) can simulate one step of a PRAM (the most convenient parallel model to program) in O((log n)2log log n) steps.",Avi Wigderson,1038
101,1001,Jean-Marie Nicolas,How to look at deductive databases,1989,"Hosts are under EMCON condition, short for Emission Control, when they are prohibited from transmitting any data. Under such condition, hosts can only receive data passively and can not acknowledge any data reception. This unidirectionality of the communication channel breaks down all currently defined transport protocols. In this paper, we present architectural alternatives that would support limited communication service under such condition and also discuss other open issues.",J. B. Bocca,1005
102,1003,M. Wallace,Mapping object-oriented concepts into relational concepts by meta-compilation in a logic programming environment,1988,"The Versatile Message Transaction Protocol (VMTP) is a transport-level protocol designed to support remote procedure call, multicast and real-time communication. The protocol is optimized for efficient page-level network file access in particular.In this paper, we describe the significant aspects of the VMTP design, including the VMTP treatment of sessions, addressing, duplicate suppression, flow control and retransmissions plus its provision for multicast. The VMTP design reflects a change in the use of computer communication as well as a change in the underlying hardware base for the next generation of communication systems. It also challenges certain established notions in the design of protocols.",J. C. Freytag,1004
103,1004,J. C. Freytag,Mapping object-oriented concepts into relational concepts by meta-compilation in a logic programming environment,1988,"The paper is concerned with efficient implementation of evolved modular and structured microprogramming. A microprogrammable architecture is presented that permits designing hierarchical complicated modular microprograms at two distinct levels: the global control and the data processing level. The architecture is based on two cooperating microprogram control units that separately store and perform control and executive microinstructions and microcode modules. The control organization of an implementing computer is presented which assures the quasi time-transparency of modular control in microprograms during the microprogram execution. This is achieved by parallel functioning of constituent control units, that permits preparing in advance addresses of executive modules referenced by control microinstructions. The efficient implementation of control statements of high level languages and microprogramming at the assembler language level for the proposed architecture are also discussed in the paper.",J. B. Bocca,1005
104,1006,Andrew V. Goldberg,A new approach to the maximum-flow problem,1988,"All previously known efficient maximum-flow algorithms work by finding augmenting paths, either one path at a time (as in the original Ford and Fulkerson algorithm) or all shortest-length augmenting paths at once (using the layered network approach of Dinic). An alternative method based on the preflow concept of Karzanov is introduced. A preflow is like a flow, except that the total amount flowing into a vertex is allowed to exceed the total amount flowing out. The method maintains a preflow in the original network and pushes local flow excess toward the sink along what are estimated to be shortest paths. The algorithm and its analysis are simple and intuitive, yet the algorithm runs as fast as any other known method on dense graphs, achieving an O(n3) time bound on an n-vertex graph. By incorporating the dynamic tree data structure of Sleator and Tarjan, we obtain a version of the algorithm running in O(nm log(n2/m)) time on an n-vertex, m-edge graph. This is as fast as any known method for any graph density and faster on graphs of moderate density. The algorithm also admits efficient distributed and parallel implementations. A parallel implementation running in O(n2log n) time using n processors and O(m) space is obtained. This time bound matches that of the Shiloach-Vishkin algorithm, which also uses n processors but requires O(n2) space.",Robert E. Tarjan,1007
105,1008,Daniel D. Sleator,A locally adaptive data compression scheme,1986,"A data compression scheme that exploits locality of reference, such as occurs when words are used frequently over short intervals and then fall into long periods of disuse, is described. The scheme is based on a simple heuristic for self-organizing sequential search and on variable-length encodings of integers. We prove that it never performs much worse than Huffman coding and can perform substantially better; experiments on real files show that its performance is usually quite close to that of Huffman coding. Our scheme has many implementation advantages: it is simple, allows fast encoding and decoding, and requires only one pass over the data to be compressed (static Huffman coding takes two passes).",Wolfgang Bibel,1000
106,1007,Robert E. Tarjan,Planar point location using persistent search trees,1986,"A classical problem in computational geometry is the planar point location problem. This problem calls for preprocessing a polygonal subdivision of the plane defined by n line segments so that, given a sequence of points, the polygon containing each point can be determined quickly on-line. Several ways of solving this problem in O(log n) query time and O(n) space are known, but they are all rather complicated. We propose a simple O(log n)-query-time, O(n)-space solution, using persistent search trees. A persistent search tree differs from an ordinary search tree in that after an insertion or deletion, the old version of the tree can still be accessed. We develop a persistent form of binary search tree that supports insertions and deletions in the present and queries in the past. The time per query or update is O(log m), where m is the total number of updates, and the space needed is O(1) per update. Our planar point location algorithm is an immediate application of this data structure. The structure also provides an alternative to Chazelle's """"hive graph"""" structure, which has a variety of applications in geometric retrieval.",Jean-Marie Nicolas,1001
107,1012,Françis X. Sillion,A global illumination solution for general reflectance distributions,1991,"A general light transfer simulation algorithm for environments composed of materials with arbitrary reflectance functions is presented. This algorithm removes the previous practical restriction to ideal specular and/or ideal diffuse environments, and supports complex physically based reflectance distributions, This is accomplished by extending previous two-pass ray-casting radiosity approaches to handle non-uniform intensity distributions, and resolving all possible energy transfers between sample points. An implementation is described based on a spherical harmonic decomposition for encoding both bidirectional reflectance distribution functions for materials, and directional intensity distributions for illuminated surfaces. The method compares favorably with experimental measurements.",Donald P. Greenberg,1018
108,1013,Greg Turk,Interactive simulation in a multi-person virtual world,1992,"A multi-user Virtual World has been implemented combining a flexible-object simulator with a multisensory user interface, including hand motion and gestures, speech input and output, sound output, and 3-D stereoscopic graphics with head-motion parallax. The implementation is based on a distributed client/server architecture with a centralized Dialogue Manager. The simulator is inserted into the Virtual World as a server. A discipline for writing interaction dialogues provides a clear conceptual hierarchy and the encapsulation of state. This hierarchy facilitates the creation of alternative interaction scenarios and shared multiuser environment.",Lawrence Koved,1016
109,1007,Robert E. Tarjan,Faster algorithms for the shortest path problem,1990,"Efficient implementations of Dijkstra's shortest path algorithm are investigated. A new data structure, called the radix heap, is proposed for use in this algorithm. On a network with n vertices, m edges, and nonnegative integer arc costs bounded by C, a one-level form of radix heap gives a time bound for Dijkstra's algorithm of O(m + n log C). A two-level form of radix heap gives a bound of O(m + n log C/log log C). A combination of a radix heap and a previously known data structure called a Fibonacci heap gives a bound of O(m + na @@@@log C). The best previously known bounds are O(m + n log n) using Fibonacci heaps alone and O(m log log C) using the priority queue structure of Van Emde Boas et al. [ 17].",Kurt Mehlhorn,1017
110,1016,Lawrence Koved,Design for interactive performance in a virtual laboratory,1990,"In recent years, a number of research groups have implemented various versions of virtual world concept [2, 4, 6, 7]. A common thread among these virtual worlds is a direct manipulation user interface paradigm based on a glove device with the position and orientation of the hand registered by a tracking device. To explore this paradigm, a new project at IBM Research was started in 1989 to build a virtual laboratory for scientists and engineers. Our first step is to integrate the glove and space tracking devices with the real time graphics on a graphics superworkstation. A simple bouncing ball virtual world has been created to test underlying software and fine tune interactive performance.Our initial emphasis is placed on understanding the limitations of various system components and getting the best interactive performance from the system. With current state of technology, the glove and tracking devices can generate much more data than the graphics update process can utilize. Both the rendering process and the processes handling the device serial ports are CPU intensive. Our first design problem is how to distribute the processing and match the incoming data rates of input devices with the update rate of the graphics. After a new position from the tracker is received by the graphics, it is displayed only at the next frame update time giving the appearance that the hand image always lags behind the motion of the real hand. Our second design problem is to use techniques to compensate for this inherent lag time. This abstract describes the specific approaches we use to solve these problems and some useful insight gained in experimenting with lag time reduction by position prediction.",J. B. Bocca,1005
111,1017,Kurt Mehlhorn,On simultaneous inner and outer approximation of shapes,1990,"For compact Euclidean bodies P, Q, we define &lgr;(P, Q) to be smallest ratio r/s where r 0, s 0 satisfy sQ? ? P ? rQ?. Here sQ denotes a scaling of Q by factor s, and Q?, Q? are some translates of Q. This function &lgr; gives us a new distance function between bodies which, unlike previously studied measures, is invariant under affine transformations. If homothetic bodies are identified, the logarithm of this function is a metric. (Two bodies are homothetic if one can be obtained from the other by scaling and translation).For integer &kgr; ? 3, define &lgr;(&kgr;) to be the minimum value such that for each convex polygon P there exists a convex &kgr;-gon Q with &lgr;(P, Q) ? &lgr;(&kgr;). Among other results, we prove that 2.118… ? &lgr;(3) ? 2.25 and &lgr;(&kgr;) = 1 + &THgr;(&kgr;-2). We give an &Ogr;(n2 log2 n) time algorithm which for any input convex n-gon P, finds a triangle T that minimizes &lgr;(T, P) among triangles. But in linear time, we can find a triangle t with &lgr;(t, P) ? 2.25.Our study is motivated by the attempt to reduce the complexity of the polygon containment problem, and also the motion planning problem. In each case, we describe algorithms which will run faster when certain implicit slackness parameters of the input are bounded away from 1. These algorithms illustrate a new algorithmic paradigm in computational geometry for coping with complexity.",Rudolf Fleischer,1025
112,1019,John M. Airey,Towards image realism with interactive update rates in complex virtual building environments,1990,"Two strategies, pre-computation before display and adaptive refinement during display, are used to combine interactivity with high image quality in a virtual building simulation. Pre-computation is used in two ways. The hidden-surface problem is partially solved by automatically pre-computing potentially visible sets of the model for sets of related viewpoints. Rendering only the potentially visible subset associated with the current viewpoint, rather than the entire model, produces significant speedups on real building models. Solutions for the radiosity lighting model are pre-computed for up to twenty different sets of lights. Linear combinations of these solutions can be manipulated in real time. We use adaptive refinement to trade image realism for interactivity as the situation requires. When the user is stationary we replace a coarse model using few polygons with a more detailed model. Image-level linear interpolation smooths the transition between differing levels of image realism.","Frederick P. Brooks, Jr.",1022
113,1023,Ware Myers,Software Pivotal to Strategic Defense,1989,"The role of software in the success of the Strategic Defense System, as reflected in the keynote address and papers at the Second International Conference on Software for Strategic Systems, in Huntsville, Alabama, October 26, 1988, is discussed. The dominant theme is that making huge aggregations of weapons work together depends on well-conceived architecture and battle-scale simulation, which takes increased investment in software environments. The issues of software reliability, software engineering environments, simulation, and software productivity are examined. An approach to software engineering called the Ada process model is defined and characterized by a set of guidelines.",Ware Myers,1023
114,1026,Robert L. Brown,Advanced Operating Systems,1984,First Page of the Article,Peter J. Denning,1028
115,1029,Barbara Simons,An ACM response: the scope and directions of Computer Science,1991,"The National Research Council's Computer Science and Telecommunications Board (CSTB) chartered a two-year study on the scope and directions of computer science. As part of this study, ACM was asked to provide input on three important questions, the answers to which could have significant impact on the future direction of our discipline and profession.",A. Joe Turner,1093
116,1028,Peter J. Denning,The evolution of parallel processing,1985,"As everyone knows, the computer industry is passing through a period of great change. I was speaking recently to a senior executive in one of the large companies vigorously working to meet the developing situation. The question he posed was: “Could it have been predicted?”",Peter J. Denning,1028
117,1032,Eli Upfal,How to share memory in a distributed system,1987,"The power of shared-memory in models of parallel computation is studied, and a novel distributed data structure that eliminates the need for shared memory without significantly increasing the run time of the parallel computation is described. More specifically, it is shown how a complete network of processors can deterministically simulate one PRAM step in O(log n/(log log n)2) time when both models use n processors and the size of the PRAM's shared memory is polynomial in n. (The best previously known upper bound was the trivial O(n)). It is established that this upper bound is nearly optimal, and it is proved that an on-line simulation of T PRAM steps by a complete network of processors requires &OHgr;(T(log n/ log log n)) time.A simple consequence of the upper bound is that an Ultracomputer (the currently feasible general-purpose parallel machine) can simulate one step of a PRAM (the most convenient parallel model to program) in O((log n)2log log n) steps.",Avi Wigderson,1038
118,1032,Eli Upfal,A tradeoff between space and efficiency for routing tables,1988,"Two conflicting goals play a crucial role in the design of routing schemes for communication networks. A routing scheme should use as short as possible paths for routing messages in the network, while keeping the routing information stored in the processors' local memory as succinct as possible. The efficiency of a routing scheme is measured in terms of its stretch factor - the maximum ratio between the length of a route computed by the scheme and that of a shortest path connecting the same pair of vertices.Most previous work has concentrated on finding good routing schemes (with a small fixed stretch factor) for special classes of network topologies. In this work we study the problem for general networks, and look at the entire range of possible stretch factors. The results exhibit a tradeoff between the efficiency of a routing scheme and its space requirements. We present almost tight upper and lower bounds for this tradeoff. Specifically, we prove that any routing scheme for general n-vertex networks that achieves a stretch factor k ? 1 must use a total of &OHgr;(n1+1/2k+4) bits of routing information in the networks. This lower bound is complemented by a family H(k) of hierarchical routing schemes (for every fixed k ? 1), which guarantee a stretch factor of &Ogr;(k), require storing a total of &Ogr;(n1+1/k) bits of routing information in the network and name the vertices with &Ogr;(log2 n)-bit names.",David Peleg,1040
119,1036,Sampath Kannan,Implicit representation of graphs,1988,"How to represent a graph in memory is a fundamental data structuring question. In the usual representations of an n-node graph, the names of the nodes (i.e. integers from 1 to n) betray nothing about the graph itself. Indeed, the names (or labels) on the n nodes are just logn bit place holders to allow data on the edges to code for the structure of the graph. In our scenario, there is no such waste. By assigning &Ogr;(logn) bit labels to the nodes, we completely code for the structure of the graph, so that given the labels of two nodes we can test if they are adjacent in time linear in the size of the labels. Furthermore, given an arbitrary original labeling of the nodes, we can find structure coding labels (as above) that are no more than a small constant factor larger than the original labels. These notions are intimately related to vertex induced universal graphs of polynomial size. For example, we can label planar graphs with structure coding labels of size n. This implies the existence of a graph with n4 nodes that contains all n-node planar graphs as vertex induced subgraphs (It was not previously known that this class had polynomial sized universal graphs). The theorems on finite graphs extend to a theorem about the constrained labeling of infinite graphs.",Moni Naor,1076
120,1037,Manuel Blum,Non-interactive zero-knowledge and its applications,1988,"We show that interaction in any zero-knowledge proof can be replaced by sharing a common, short, random string. We use this result to construct the first public-key cryptosystem secure against chosen ciphertext attack.",Paul Feldman,1043
121,1038,Avi Wigderson,A fast parallel algorithm for the maximal independent set problem,1985,"A parallel algorithm is presented that accepts as input a graph G and produces a maximal independent set of vertices in G. On a P-RAM without the concurrent write or concurrent read features, the algorithm executes in O((log n)4) time and uses O((n/(log n))3) processors, where n is the number of vertices in G. The algorithm has several novel features that may find other applications. These include the use of balanced incomplete block designs to replace random sampling by deterministic sampling, and the use of a “dynamic pigeonhole principle” that generalizes the conventional pigeonhole principle.",Richard M. Karp,1094
122,1041,Srinivas Devadas,Verification of interacting sequential circuits,1991,"The problem of verifying the equivalence of interacting finite state machines (FSMs) described at the logic level is addressed. The problem is formulated as that of checking for the equivalence of the reset/starting states of the two FSMs. First, separate sum-of-product representations of the ON-sets and OFF-sets of each of the flip-flop inputs and primary outputs of the sequential circuit, are extracted using the PODEM algorithm. We describe a fast algorithm for state differentiation based on this representation. The input as well as the state space is implicitly enumerated through a process of repeated cube intersections to generate the State Transition Graph (STG).In contrast to previous approaches, this algorithm can be efficiently generalized for verifying distributed-style specifications of interacting sequential circuits, exploiting the nature of the interconnection topology. Pipeline latches in a distributed-style specification typically do not add complexity to the sequential behavior of a circuit, but greatly add to the complexity of traditional approaches to verifying sequential circuits. Pipeline latches are easily incorporated into our generalized, hierarchical verification strategy whereby the states of pipeline latches can be implicitly enumerated.Experimental results indicate the superior efficiency of this approach as compared to previous approaches for FSM verification. It is possible to verify examples with more than 1050 states.",David Peleg,1040
123,1043,Paul Feldman,Non-interactive zero-knowledge and its applications,1988,"We show that interaction in any zero-knowledge proof can be replaced by sharing a common, short, random string. We use this result to construct the first public-key cryptosystem secure against chosen ciphertext attack.",Paul Feldman,1043
124,1044,Abdul A. Malik,Reduced offsets for two-level multi-valued logic minimization,1991,"The approaches to two-level logic minimization can be classified into two groups: those that use tautology for expansion of cubes and those that use the offset. Tautology based schemes are generally slower and often give somewhat inferior results, because of a limited global picture of the way in which the cube can be expanded. If the offset is used, usually the expansion can be done quickly and in a more global way because it is easier to see effective directions of expansion. The problem with this approach is that there are many functions that have a reasonable size onset and don't care set but the offset is unreasonably large. It was recently shown that for the minimization of such Boolean functions, a new approach using reduced offsets, provides the same global picture and can be computed much faster. In this paper we extend reduced offsets to logic functions with multi-valued inputs.",A. Richard Newton,1046
125,1041,Srinivas Devadas,Verification of interacting sequential circuits,1991,"The problem of verifying the equivalence of interacting finite state machines (FSMs) described at the logic level is addressed. The problem is formulated as that of checking for the equivalence of the reset/starting states of the two FSMs. First, separate sum-of-product representations of the ON-sets and OFF-sets of each of the flip-flop inputs and primary outputs of the sequential circuit, are extracted using the PODEM algorithm. We describe a fast algorithm for state differentiation based on this representation. The input as well as the state space is implicitly enumerated through a process of repeated cube intersections to generate the State Transition Graph (STG).In contrast to previous approaches, this algorithm can be efficiently generalized for verifying distributed-style specifications of interacting sequential circuits, exploiting the nature of the interconnection topology. Pipeline latches in a distributed-style specification typically do not add complexity to the sequential behavior of a circuit, but greatly add to the complexity of traditional approaches to verifying sequential circuits. Pipeline latches are easily incorporated into our generalized, hierarchical verification strategy whereby the states of pipeline latches can be implicitly enumerated.Experimental results indicate the superior efficiency of this approach as compared to previous approaches for FSM verification. It is possible to verify examples with more than 1050 states.",Paul Feldman,1043
126,1049,Alberto Sangiovanni-Vincentelli,Computer architecture issues in circuit simulation,1988,"The computational complexity of approximating omega (G), the size of the largest clique in a graph G, within a given factor is considered. It is shown that if certain approximation procedures exist, then EXPTIME=NEXPTIME and NP=P.",Alberto Sangiovanni-Vincentelli,1049
127,1050,J. C. van Vliet,An evaluation of tagging,1985,"Tag handling accounts for a substantial amount of execution cost in latently typed languages such as Common LISP and Scheme, espicially on architectures that provide no special hardware support.",H. M. Gladney,1052
128,1052,H. M. Gladney,Data replicas in distributed information services,1989,"In an information distribution network in which records are repeatedly read, it is cost-effective to keep read-only copies in work locations. This paper presents a method of updating replicas that need not be immediately synchronized with the source data or with each other. The method allows an arbitrary mapping from source records to replica records. It is fail-safe, maximizes workstation autonomy, and is well suited to a network with slow, unreliable, and/or expensive communications links.The algorithm is a manipulation of queries, which are represented as short encodings. When a response is generated, we record which portion of the source database was used. Later, when the source data are updated, this information is used to identify obsolete replicas. For each workstation, the identity of obsolete replicas is saved until a workstation process asks for this information. This workstation process deletes each obsolete replica, and replaces it by an up-to-date version either promptly or the next time the application asks for this particular item. Throughout, queries are grouped so that the impact of each source update transaction takes effect atomically at each workstation.Optimizations of the basic algorithm are outlined. These overlap change dissemination with user service, allow the mechanism to be hidden within the data delivery subsystem, and permit very large networks.",H. M. Gladney,1052
129,1053,M. Girault,A generalized birthday attack,1988,"We generalize the birthday attack presented by Coppersmith at Crypto'85 which defrauded a Davies-Price message authentication scheme. We first study the birthday paradox and a variant for which some convergence results and related bounds are provided. Secondly, we generalize the Davies-Price scheme and show how the Coppersmith attack can be extended to this case. As a consequence, the case p=4 with DES (important when RSA with a 512-bit modulus is used for signature) appears not to be secure enough.",M. Girault,1053
130,1057,Mariano P. Consens,The G+/GraphLog Visual Query System,1990,"The video presentation “The G+/GraphLog Visual Query System” gives an overview of the capabilities of the ongoing implementation of the G+ Visual Query System for visualizing both data and queries as graphs. The system provides an environment for expressing queries in GraphLog [Con89, CM89, CM90], as well as for browsing, displaying and editing graphs. The visual query system also supports displaying the answers in several different ways.Graphs are a very natural representation for data in many application domains, for example, transportation networks, project scheduling, parts hierarchies, family trees, concept hierarchies, and Hypertext. From a broader perspective, many databases can be naturally viewed as graphs. In particular, any relational database in which we can identify one or more sets of objects of interest and relationships between them can be represented by mapping these objects into nodes and relationships into edges. In the case of semantic and object-oriented databases, there is a natural mapping of objects to nodes and attributes to edges.GraphLog is a visual query language, based on a graph representation of both data and queries, that has evolved from the earlier language G+ [CMW87, CMW89, MW89]. GraphLog queries ask for patterns that must be present or absent in the database graph. Each such pattern, called a query graph, defines new edges that are added to the graph whenever the pattern is found. GraphLog queries are sets of query graphs, called graphical queries. If, when looking at a query graph in a graphical query, we do not find an edge label in the database, then there must exist another query graph in the graphical query defining that edge. The language also supports computing aggregate functions and summarizing along paths.The G+ Visual Query System is currently implemented in Smalltalk-80™, and runs on Sun 3, Sun 4 and Macintosh II workstations. A Graph Editor is available for editing query graphs and displaying database graphs. It supports graph “cutting and pasting”, as well as text editing of node and edge labels, node and edge repositioning and re-shaping, storage and retrieval of graphs as text files, etc. Automatic graph layout is also provided. For editing collections of graphs (such as graphical queries) a Graph Browser is available.The first answer mode supported by the G+ Visual Query System is to return as the result of a GraphLog query a graph with the new edges defined by the graphical query added to the database graph.An alternative way of visualizing answers is by high-lighting on the database graph, one at a time, the paths (or just the nodes) described by the query. This mode is particularly useful to locate interesting starting points for browsing.Rather than viewing the answers superimposed on the database graph, the user may choose to view them in a Graph Browser. The Graph Browser contains the set of subgraphs of the database graph that were found to satisfy the query.Finally, the user may select to collect all the subgraphs of the database graph that satisfy the query together into one new graph. This graph (as well as any other result graph from any of the above mentioned answer modes) in turn may be queried, providing a mechanism for iterative filtering of irrelevant information until a manageable subgraph is obtained.",Alberto O. Mendelzon,1059
131,1057,Mariano P. Consens,Low complexity aggregation in GraphLog and Datalog,1990,"We present facilities for computing aggregate functions over sets of tuples and along paths in a database graph.We show how Datalog can be extended to compute a large class of queries with aggregates without incurring the large expense of a language with general set manipulation capabilities. In particular, we aim for queries that can be executed efficiently in parallel, using the class NC and its various subclasses as formal models of low parallel complexity.Our approach retains the standard relational notion of relations as sets of tuples, not requiring the introduction of multisets. For the case where no rules are recursive, the language is exactly as expressive as Klug's first order language with aggregates. We show that this class of non-recursive programs cannot express transitive closure (unless LOGSPACE=NLOGSPACE), thus providing evidence for a widely believed but never proven folk result. We also study the expressive power and complexity of languages that support aggregation over recursion.We then describe how these facilities, as well as manipulating the length of paths in database graphs, are incorporated into our visual query language GraphLog. While GraphLog could easily be extended to handle all the queries described above, we prefer to restrict the language in a natural way to avoid explicit recursion; all recursion is expressed as transitive closure. We show that this guarantees all expressible queries are in NC. We analyze other proposals and show that they can express queries that are logspace-complete for P and thus unlikely to be parallelizable efficiently.",Mariano P. Consens,1057
132,1059,Alberto O. Mendelzon,Answering queries on embedded-complete database schemes,1987,"It has been observed that, for some database schemes, users may have difficulties retrieving correct information, even for simple queries. The problem occurs when some implicit “piece” of information, defined on some subset of a relation scheme, is not explicitly represented in the database state. In this situation, users may be required to know how the state and the constraints interact before they can retrieve the information correctly.In this paper, the formal notion of embedded-completeness is proposed, and it is shown that schemes with this property avoid the problem described above. A polynomial-time algorithm is given to test whether a database scheme is independent and embedded-complete. Under the assumption of independence, it is shown that embedded-complete schemes allow efficient computation of optimal relational algebra expressions equivalent to the X-total projection, for any set of attributes X.",M. Wallace,1003
133,1063,Yanjun Zhang,On better heuristic for euclidean Steiner minimum trees (extended abstract),1991,Finding a shortest network interconnecting a given set of points in the Euclidean plane (a Steiner minimum tree) is known to be NP-hard. It is shown that there exists a polynomial-time heuristic with a performance ratio bigger than square root 3/2.,Yanjun Zhang,1063
134,1064,Yonatan Aumann,Asymptotically optimal PRAM emulation on faulty hypercubes (extended abstract),1991,"A scheme for emulating the parallel random access machine (PRAM) on a faulty hypercube is presented. All components of the hypercube, including the memory modules, are assumed to be subject to failure. The faults may occur at any time during the emulation and the system readjusts dynamically. The scheme, which rests on L.G. Valiant's BSP model (1990), is the first to achieve optimal and work-preserving PRAM emulation on a dynamically faulty network.",Michael Ben-Or,1056
135,1056,Michael Ben-Or,Completeness theorems for non-cryptographic fault-tolerant distributed computation,1988,"Every function of n inputs can be efficiently computed by a complete network of n processors in such a way that:If no faults occur, no set of size t n/2 of players gets any additional information (other than the function value),Even if Byzantine faults are allowed, no set of size t n/3 can either disrupt the computation or get additional information.Furthermore, the above bounds on t are tight!",Shafi Goldwasser,1068
136,1068,Shafi Goldwasser,New paradigms for digital signatures and message authentication based on non-interactive zero knowledge proofs,1989,Using non-interactive zero knowledge proofs we provide a simple new paradigm for digital signing and message authentication secure against adaptive chosen message attack.For digital signatures we require that the non-interactive zero knowledge proofs be publicly verifiable: they should be checkable by anyone rather than directed at a particular verifier. We accordingly show how to implement noninteractive zero knowledge proofs in a network which have the property that anyone in the network can individually check correctness while the proof is zero knowledge to any sufficiently small coalition. This enables us to implement signatures which are history independent.,Mihir Bellare,1071
137,1056,Michael Ben-Or,Multi-prover interactive proofs: how to remove intractability assumptions,1988,"Quite complex cryptographic machinery has been developed based on the assumption that one-way functions exist, yet we know of only a few possible such candidates. It is important at this time to find alternative foundations to the design of secure cryptography. We introduce a new model of generalized interactive proofs as a step in this direction. We prove that all NP languages have perfect zero-knowledge proof-systems in this model, without making any intractability assumptions.The generalized interactive-proof model consists of two computationally unbounded and untrusted provers, rather than one, who jointly agree on a strategy to convince the verifier of the truth of an assertion and then engage in a polynomial number of message exchanges with the verifier in their attempt to do so. To believe the validity of the assertion, the verifier must make sure that the two provers can not communicate with each other during the course of the proof process. Thus, the complexity assumptions made in previous work, have been traded for a physical separation between the two provers.We call this new model the multi-prover interactive-proof model, and examine its properties and applicability to cryptography.",Michael Ben-Or,1056
138,1071,Mihir Bellare,A technique for upper bounding the spectral norm with applications to learning,1992,"We present a general technique to upper bound the spectral norm of an arbitrary function. At the heart of our technique is a theorem which shows how to obtain an upper bound on the spectral norm of a decision tree given the spectral norms of the functions in the nodes of this tree. The theorem applies to trees whose nodes may compute any boolean functions. Applications are to the design of efficient learning algorithms and the construction of small depth threshold circuits (or neural nets). In particular, we present polynomial time algorithms for learning O(log n) clause DNF formulas and various classes of decision trees, all under the uniform distribution with membership queries.",Mihir Bellare,1071
139,1072,Donald Beaver,Multiparty protocols tolerating half faulty processors,1989,"We show that a complete broadcast network of n processors can evaluate any function f(x1,..., xn) at private inputs supplied by each processor, revealing no information other than the result of the function, while tolerating up to t maliciously faulty parties for 2t n. This improves the previous bound of 3t n on the tolerable number of faults [BG W88, CCD88]. We demonstrate a resilient method to multiply secretly shared values without using unproven cryptographic assumptions. The crux of our method is a new, non-cryptographic zero-knowledge technique which extends verifiable secret sharing to allow proofs based on secretly shared values. Under this method, a single party can secretly share values v1,...vm along with another secret w = P(v1,...,vm), where P is any polynomial size circuit; and she can prove to all other parties that w = P(v1,..., vm), without revealing w or any other information. Our protocols allow an exponentially small chance of error, but are provably optimal in their resilience against Byzantine faults. Furthermore, our solutions use operations over exponentially large fields, greatly reducing the amount of interaction necessary for computing natural functions.",Donald Beaver,1072
140,1076,Moni Naor,Bit commitment using pseudo-randomness (extended abstract),1989,"We show how a pseudo-random generator can provide a bit commitment protocol. We also analyze the number of bits communicated when parties commit to many bits simultaneously, and show that the assumption of the existence of pseudorandom generators suffices to assure amortized O(1) bits of communication per bit commitment.",Moni Naor,1076
141,1076,Moni Naor,Amortized communication complexity (Preliminary version),1991,"The authors study the direct sum problem with respect to communication complexity: Consider a function f: D to (0, 1), where D contained in (0, 1)/sup n/*(0, 1)/sup n/. The amortized communication complexity of f, i.e. the communication complexity of simultaneously computing f on l instances, divided by l is studied. The authors present, both in the deterministic and the randomized model, functions with communication complexity Theta (log n) and amortized communication complexity O(1). They also give a general lower bound on the amortized communication complexity of any function f in terms of its communication complexity C(f).",Moni Naor,1076
142,1079,Eyal Kushilevitz,Randomized mutual exclusion algorithms revisited,1992,"In [4] a randomized algorithm for mutual exclusion with bounded waiting, employing a logarithmic sized shared variable, was given. Saias and Lynch [5] pointed out that the adversary scheduler postulated in the above paper can observe the behavior of processes in the interval between an opening of the critical section and the next closing of the critical section. it can then draw conclusions about values of their local variables as well as the value of the randomized round number component of the shared variable, and arrange the schedule so as to discriminate against a chosen process. This invalidates the claimed properties of the algorithm.In the present paper the algorithm in [4] is modified, using the ideas of [4], so as to overcome this difficulty, obtaining essentially the same results. Thus, as in [4], randomization yields simple algorithms for mutual-exclusion with bounded waiting, employing a shared variable of considerably smaller size than the lower-bound established in [1] for deterministic algorithms.",Michael O. Rabin,1080
143,1080,Michael O. Rabin,"Efficient dispersal of information for security, load balancing, and fault tolerance",1989,"An Information Dispersal Algorithm (IDA) is developed that breaks a file F of length L = &uharl; F&uharr; into n pieces Fi, l ? i ? n, each of length &uharl;Fi&uharr; = L/m, so that every m pieces suffice for reconstructing F. Dispersal and reconstruction are computationally efficient. The sum of the lengths &uharl;Fi&uharr; is (n/m) · L. Since n/m can be chosen to be close to l, the IDA is space efficient. IDA has numerous applications to secure and reliable storage of information in computer networks and even on single disks, to fault-tolerant and efficient transmission of information in networks, and to communications between processors in parallel computers. For the latter problem provably time-efficient and highly fault-tolerant routing on the n-cube is achieved, using just constant size buffers.",Michael O. Rabin,1080
144,1082,Mihalis Yannakakis,A polynomial algorithm for the min-cut linear arrangement of trees,1985,An algorithm is presented that finds a min-cut linear arrangement of a tree in O(n log n) time. An extension of the algorithm determines the number of pebbles needed to play the black and white pebble game on a tree.,Mihalis Yannakakis,1082
145,1087,H. E. Lü,A comment on “a fast parallel algorithm for thinning digital patterns,1986,"A fast parallel thinning algorithm for digital patterns is presented. This algorithm is an improved version of the algorithms introduced by Zhang and Suen [5] and Stefanelli and Rosenfeld [3]. An experiment using an Apple II and an Epson printer was conducted. The results show that the improved algorithm overcomes some of the disadvantages found in [5] by preserving necessary and essential structures for certain patterns which should not be deleted and maintains very fast speed, from about 1.5 to 2.3 times faster than the four-step and two-step methods described in [3] although the resulting skeletons look basically the same.",P. S. P. Wang,1086
146,1088,James H. Morris,Andrew: a distributed personal computing environment,1986,"The Information Technology Center (ITC), a collaborative effort between IBM and Carnegie-Mellon University, is in the process of creating Andrew, a prototype computing and communication system for universities. This article traces the origins of Andrew, discusses its goals and strategies, and gives an overview of the current status of its implementation and usage.",Mahadev Satyanarayanan,1089
147,1090,Mee-Chow Chiang,"Experience with mean value analysis model for evaluating shared bus, throughput-oriented multiprocessors",1991,"We report on our experience with the accuracy of mean value analysis analytical models for evaluating shared bus multiprocessors operating in a throughput-oriented environment. Having developed separate models for multiprocessors with circuit switched and split transaction, pipelined (packet switched) buses, wc compare the results of the models with those of an actual trace-driven simulation for 5,376 multiprocessor configurations.We find that the analytical models are accurate in predicting the individual processor throughputs and partial bus utilizations. For processor throughput, the difference between the results of the models and simulation are within 1% for 75% of the cases and within 3% in 94% of all cases. For partial bus utilization the model results are with 1% of simulation results in 70% of all cases and within 3% in 92% of all cases. The models are less accurate in predicting cache miss latency.",Gurindar S. Sohi,1091
148,1091,Gurindar S. Sohi,Tradeoffs in instruction format design for horizontal architectures,1989,"With recent improvements in software techniques and the enhanced level of fine grain parallelism made available by such techniques, there has been an increased interest in horizontal architectures and large instruction words that are capable of issuing more that one operation per instruction. This paper investigates some issues in the design of such instruction formats. We study how the choice of an instruction format is influenced by factors such as the degree of pipelining and the instruction's view of the register file. Our results suggest that very large instruction words capable of issuing one operation to each functional unit resource in a horizontal architecture may be overkill. Restricted instruction formats with limited operation issuing capabilities are capable of providing similar performance (measured by the total number of time steps) with significantly less hardware in many cases.",Gurindar S. Sohi,1091
149,1093,A. Joe Turner,An ACM response: the scope and directions of Computer Science,1991,"The National Research Council's Computer Science and Telecommunications Board (CSTB) chartered a two-year study on the scope and directions of computer science. As part of this study, ACM was asked to provide input on three important questions, the answers to which could have significant impact on the future direction of our discipline and profession.",Paul Feldman,1043
150,1040,David Peleg,Concurrent dynamic logic,1987,"In this paper concurrent dynamic logic (CDL) is introduced as an extension of dynamic logic tailored toward handling concurrent programs. Properties of CDL are discussed, both on the propositional and first-order level, and the extension is shown to possess most of the desirable properties of DL. Its relationships with the &mgr;-calculus, game logic, DL with recursive procedures, and PTIME are further explored, revealing natural connections between concurrency, recursion, and alternation.",David Peleg,1040
151,1038,Avi Wigderson,Multi-prover interactive proofs: how to remove intractability assumptions,1988,"Quite complex cryptographic machinery has been developed based on the assumption that one-way functions exist, yet we know of only a few possible such candidates. It is important at this time to find alternative foundations to the design of secure cryptography. We introduce a new model of generalized interactive proofs as a step in this direction. We prove that all NP languages have perfect zero-knowledge proof-systems in this model, without making any intractability assumptions.The generalized interactive-proof model consists of two computationally unbounded and untrusted provers, rather than one, who jointly agree on a strategy to convince the verifier of the truth of an assertion and then engage in a polynomial number of message exchanges with the verifier in their attempt to do so. To believe the validity of the assertion, the verifier must make sure that the two provers can not communicate with each other during the course of the proof process. Thus, the complexity assumptions made in previous work, have been traded for a physical separation between the two provers.We call this new model the multi-prover interactive-proof model, and examine its properties and applicability to cryptography.",David Peleg,1040
152,1040,David Peleg,A tradeoff between space and efficiency for routing tables,1988,"Two conflicting goals play a crucial role in the design of routing schemes for communication networks. A routing scheme should use as short as possible paths for routing messages in the network, while keeping the routing information stored in the processors' local memory as succinct as possible. The efficiency of a routing scheme is measured in terms of its stretch factor - the maximum ratio between the length of a route computed by the scheme and that of a shortest path connecting the same pair of vertices.Most previous work has concentrated on finding good routing schemes (with a small fixed stretch factor) for special classes of network topologies. In this work we study the problem for general networks, and look at the entire range of possible stretch factors. The results exhibit a tradeoff between the efficiency of a routing scheme and its space requirements. We present almost tight upper and lower bounds for this tradeoff. Specifically, we prove that any routing scheme for general n-vertex networks that achieves a stretch factor k ? 1 must use a total of &OHgr;(n1+1/2k+4) bits of routing information in the networks. This lower bound is complemented by a family H(k) of hierarchical routing schemes (for every fixed k ? 1), which guarantee a stretch factor of &Ogr;(k), require storing a total of &Ogr;(n1+1/k) bits of routing information in the network and name the vertices with &Ogr;(log2 n)-bit names.",Eli Upfal,1032
